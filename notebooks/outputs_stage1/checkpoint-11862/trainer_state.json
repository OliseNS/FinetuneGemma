{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 11862,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004215673875468994,
      "grad_norm": 0.6592884063720703,
      "learning_rate": 0.00019976375295308808,
      "loss": 1.5586,
      "step": 25
    },
    {
      "epoch": 0.008431347750937988,
      "grad_norm": 0.6036758422851562,
      "learning_rate": 0.00019934188322645968,
      "loss": 0.7879,
      "step": 50
    },
    {
      "epoch": 0.012647021626406981,
      "grad_norm": 0.5132383108139038,
      "learning_rate": 0.00019892001349983127,
      "loss": 0.702,
      "step": 75
    },
    {
      "epoch": 0.016862695501875975,
      "grad_norm": 0.39161258935928345,
      "learning_rate": 0.00019849814377320284,
      "loss": 0.7755,
      "step": 100
    },
    {
      "epoch": 0.021078369377344967,
      "grad_norm": 0.42830291390419006,
      "learning_rate": 0.00019807627404657443,
      "loss": 0.6384,
      "step": 125
    },
    {
      "epoch": 0.025294043252813963,
      "grad_norm": 0.5469884872436523,
      "learning_rate": 0.00019765440431994603,
      "loss": 0.7503,
      "step": 150
    },
    {
      "epoch": 0.029509717128282955,
      "grad_norm": 0.42612940073013306,
      "learning_rate": 0.0001972325345933176,
      "loss": 0.7441,
      "step": 175
    },
    {
      "epoch": 0.03372539100375195,
      "grad_norm": 0.5841566920280457,
      "learning_rate": 0.00019681066486668916,
      "loss": 0.729,
      "step": 200
    },
    {
      "epoch": 0.03794106487922094,
      "grad_norm": 0.5062210559844971,
      "learning_rate": 0.00019638879514006076,
      "loss": 0.6978,
      "step": 225
    },
    {
      "epoch": 0.042156738754689935,
      "grad_norm": 0.3573777973651886,
      "learning_rate": 0.00019596692541343233,
      "loss": 0.7246,
      "step": 250
    },
    {
      "epoch": 0.046372412630158934,
      "grad_norm": 0.6388674974441528,
      "learning_rate": 0.00019554505568680392,
      "loss": 0.6521,
      "step": 275
    },
    {
      "epoch": 0.050588086505627926,
      "grad_norm": 0.3178512752056122,
      "learning_rate": 0.00019512318596017552,
      "loss": 0.7631,
      "step": 300
    },
    {
      "epoch": 0.05480376038109692,
      "grad_norm": 0.3294990658760071,
      "learning_rate": 0.00019470131623354708,
      "loss": 0.6467,
      "step": 325
    },
    {
      "epoch": 0.05901943425656591,
      "grad_norm": 0.458615243434906,
      "learning_rate": 0.00019427944650691868,
      "loss": 0.5709,
      "step": 350
    },
    {
      "epoch": 0.06323510813203491,
      "grad_norm": 0.5414772629737854,
      "learning_rate": 0.00019385757678029027,
      "loss": 0.7259,
      "step": 375
    },
    {
      "epoch": 0.0674507820075039,
      "grad_norm": 0.4126910865306854,
      "learning_rate": 0.00019343570705366184,
      "loss": 0.7192,
      "step": 400
    },
    {
      "epoch": 0.0716664558829729,
      "grad_norm": 0.287296861410141,
      "learning_rate": 0.0001930138373270334,
      "loss": 0.7791,
      "step": 425
    },
    {
      "epoch": 0.07588212975844189,
      "grad_norm": 0.5636021494865417,
      "learning_rate": 0.000192591967600405,
      "loss": 0.6891,
      "step": 450
    },
    {
      "epoch": 0.08009780363391088,
      "grad_norm": 0.5874422788619995,
      "learning_rate": 0.00019217009787377657,
      "loss": 0.679,
      "step": 475
    },
    {
      "epoch": 0.08431347750937987,
      "grad_norm": 0.4948570430278778,
      "learning_rate": 0.00019174822814714817,
      "loss": 0.7654,
      "step": 500
    },
    {
      "epoch": 0.08852915138484886,
      "grad_norm": 0.471447229385376,
      "learning_rate": 0.00019132635842051976,
      "loss": 0.6485,
      "step": 525
    },
    {
      "epoch": 0.09274482526031787,
      "grad_norm": 0.5673883557319641,
      "learning_rate": 0.00019090448869389133,
      "loss": 0.7328,
      "step": 550
    },
    {
      "epoch": 0.09696049913578686,
      "grad_norm": 0.48932039737701416,
      "learning_rate": 0.00019048261896726293,
      "loss": 0.6338,
      "step": 575
    },
    {
      "epoch": 0.10117617301125585,
      "grad_norm": 0.4578547775745392,
      "learning_rate": 0.0001900607492406345,
      "loss": 0.6501,
      "step": 600
    },
    {
      "epoch": 0.10539184688672484,
      "grad_norm": 0.38154685497283936,
      "learning_rate": 0.00018963887951400606,
      "loss": 0.6793,
      "step": 625
    },
    {
      "epoch": 0.10960752076219384,
      "grad_norm": 0.4200257956981659,
      "learning_rate": 0.00018921700978737766,
      "loss": 0.6818,
      "step": 650
    },
    {
      "epoch": 0.11382319463766283,
      "grad_norm": 0.3569240868091583,
      "learning_rate": 0.00018879514006074925,
      "loss": 0.6172,
      "step": 675
    },
    {
      "epoch": 0.11803886851313182,
      "grad_norm": 0.4966697096824646,
      "learning_rate": 0.00018837327033412082,
      "loss": 0.6623,
      "step": 700
    },
    {
      "epoch": 0.12225454238860081,
      "grad_norm": 0.3980119526386261,
      "learning_rate": 0.00018795140060749241,
      "loss": 0.6168,
      "step": 725
    },
    {
      "epoch": 0.12647021626406982,
      "grad_norm": 0.5363979339599609,
      "learning_rate": 0.000187529530880864,
      "loss": 0.7424,
      "step": 750
    },
    {
      "epoch": 0.1306858901395388,
      "grad_norm": 0.6548535823822021,
      "learning_rate": 0.0001871076611542356,
      "loss": 0.7289,
      "step": 775
    },
    {
      "epoch": 0.1349015640150078,
      "grad_norm": 0.46704715490341187,
      "learning_rate": 0.00018668579142760717,
      "loss": 0.6789,
      "step": 800
    },
    {
      "epoch": 0.13911723789047678,
      "grad_norm": 0.6546409726142883,
      "learning_rate": 0.00018626392170097874,
      "loss": 0.6294,
      "step": 825
    },
    {
      "epoch": 0.1433329117659458,
      "grad_norm": 0.5925196409225464,
      "learning_rate": 0.00018584205197435033,
      "loss": 0.5929,
      "step": 850
    },
    {
      "epoch": 0.1475485856414148,
      "grad_norm": 0.5094623565673828,
      "learning_rate": 0.0001854201822477219,
      "loss": 0.6491,
      "step": 875
    },
    {
      "epoch": 0.15176425951688377,
      "grad_norm": 0.5576695203781128,
      "learning_rate": 0.0001849983125210935,
      "loss": 0.6633,
      "step": 900
    },
    {
      "epoch": 0.15597993339235278,
      "grad_norm": 0.656904935836792,
      "learning_rate": 0.0001845764427944651,
      "loss": 0.5578,
      "step": 925
    },
    {
      "epoch": 0.16019560726782175,
      "grad_norm": 0.5915118455886841,
      "learning_rate": 0.00018415457306783666,
      "loss": 0.6476,
      "step": 950
    },
    {
      "epoch": 0.16441128114329076,
      "grad_norm": 0.47110211849212646,
      "learning_rate": 0.00018373270334120825,
      "loss": 0.6094,
      "step": 975
    },
    {
      "epoch": 0.16862695501875974,
      "grad_norm": 0.5799053907394409,
      "learning_rate": 0.00018331083361457982,
      "loss": 0.6819,
      "step": 1000
    },
    {
      "epoch": 0.17284262889422874,
      "grad_norm": 0.5956956744194031,
      "learning_rate": 0.0001828889638879514,
      "loss": 0.6597,
      "step": 1025
    },
    {
      "epoch": 0.17705830276969772,
      "grad_norm": 0.5662257075309753,
      "learning_rate": 0.00018246709416132298,
      "loss": 0.6691,
      "step": 1050
    },
    {
      "epoch": 0.18127397664516673,
      "grad_norm": 0.4691295921802521,
      "learning_rate": 0.00018204522443469458,
      "loss": 0.6728,
      "step": 1075
    },
    {
      "epoch": 0.18548965052063573,
      "grad_norm": 0.4611334800720215,
      "learning_rate": 0.00018162335470806615,
      "loss": 0.6886,
      "step": 1100
    },
    {
      "epoch": 0.1897053243961047,
      "grad_norm": 0.5494567155838013,
      "learning_rate": 0.00018120148498143774,
      "loss": 0.6516,
      "step": 1125
    },
    {
      "epoch": 0.19392099827157372,
      "grad_norm": 0.5572909116744995,
      "learning_rate": 0.00018077961525480934,
      "loss": 0.7555,
      "step": 1150
    },
    {
      "epoch": 0.1981366721470427,
      "grad_norm": 0.615109920501709,
      "learning_rate": 0.0001803577455281809,
      "loss": 0.5759,
      "step": 1175
    },
    {
      "epoch": 0.2023523460225117,
      "grad_norm": 0.5771766304969788,
      "learning_rate": 0.0001799358758015525,
      "loss": 0.7107,
      "step": 1200
    },
    {
      "epoch": 0.20656801989798068,
      "grad_norm": 0.5355958938598633,
      "learning_rate": 0.00017951400607492407,
      "loss": 0.6695,
      "step": 1225
    },
    {
      "epoch": 0.2107836937734497,
      "grad_norm": 0.5520287156105042,
      "learning_rate": 0.00017909213634829564,
      "loss": 0.6799,
      "step": 1250
    },
    {
      "epoch": 0.2149993676489187,
      "grad_norm": 0.5686078667640686,
      "learning_rate": 0.00017867026662166723,
      "loss": 0.6833,
      "step": 1275
    },
    {
      "epoch": 0.21921504152438767,
      "grad_norm": 0.5358233451843262,
      "learning_rate": 0.00017824839689503883,
      "loss": 0.7612,
      "step": 1300
    },
    {
      "epoch": 0.22343071539985668,
      "grad_norm": 0.44661596417427063,
      "learning_rate": 0.0001778265271684104,
      "loss": 0.6023,
      "step": 1325
    },
    {
      "epoch": 0.22764638927532566,
      "grad_norm": 0.5983580350875854,
      "learning_rate": 0.000177404657441782,
      "loss": 0.6756,
      "step": 1350
    },
    {
      "epoch": 0.23186206315079466,
      "grad_norm": 0.49695885181427,
      "learning_rate": 0.00017698278771515358,
      "loss": 0.6665,
      "step": 1375
    },
    {
      "epoch": 0.23607773702626364,
      "grad_norm": 0.5483479499816895,
      "learning_rate": 0.00017656091798852515,
      "loss": 0.66,
      "step": 1400
    },
    {
      "epoch": 0.24029341090173265,
      "grad_norm": 0.6481131911277771,
      "learning_rate": 0.00017613904826189675,
      "loss": 0.5954,
      "step": 1425
    },
    {
      "epoch": 0.24450908477720162,
      "grad_norm": 0.5621160268783569,
      "learning_rate": 0.00017571717853526831,
      "loss": 0.707,
      "step": 1450
    },
    {
      "epoch": 0.24872475865267063,
      "grad_norm": 0.5550326108932495,
      "learning_rate": 0.00017529530880863988,
      "loss": 0.6561,
      "step": 1475
    },
    {
      "epoch": 0.25294043252813964,
      "grad_norm": 0.4128023684024811,
      "learning_rate": 0.00017487343908201148,
      "loss": 0.6088,
      "step": 1500
    },
    {
      "epoch": 0.25715610640360864,
      "grad_norm": 0.37165990471839905,
      "learning_rate": 0.00017445156935538307,
      "loss": 0.5795,
      "step": 1525
    },
    {
      "epoch": 0.2613717802790776,
      "grad_norm": 0.3601559102535248,
      "learning_rate": 0.00017402969962875464,
      "loss": 0.6203,
      "step": 1550
    },
    {
      "epoch": 0.2655874541545466,
      "grad_norm": 0.578576922416687,
      "learning_rate": 0.00017360782990212623,
      "loss": 0.597,
      "step": 1575
    },
    {
      "epoch": 0.2698031280300156,
      "grad_norm": 0.590861976146698,
      "learning_rate": 0.00017318596017549783,
      "loss": 0.689,
      "step": 1600
    },
    {
      "epoch": 0.2740188019054846,
      "grad_norm": 0.5369037389755249,
      "learning_rate": 0.0001727640904488694,
      "loss": 0.6618,
      "step": 1625
    },
    {
      "epoch": 0.27823447578095356,
      "grad_norm": 0.5017134547233582,
      "learning_rate": 0.00017234222072224096,
      "loss": 0.6494,
      "step": 1650
    },
    {
      "epoch": 0.28245014965642257,
      "grad_norm": 0.6125921010971069,
      "learning_rate": 0.00017192035099561256,
      "loss": 0.6709,
      "step": 1675
    },
    {
      "epoch": 0.2866658235318916,
      "grad_norm": 0.484631210565567,
      "learning_rate": 0.00017149848126898415,
      "loss": 0.6782,
      "step": 1700
    },
    {
      "epoch": 0.2908814974073606,
      "grad_norm": 0.6422034502029419,
      "learning_rate": 0.00017107661154235572,
      "loss": 0.7497,
      "step": 1725
    },
    {
      "epoch": 0.2950971712828296,
      "grad_norm": 0.577578604221344,
      "learning_rate": 0.00017065474181572732,
      "loss": 0.6431,
      "step": 1750
    },
    {
      "epoch": 0.29931284515829853,
      "grad_norm": 0.6019136905670166,
      "learning_rate": 0.0001702328720890989,
      "loss": 0.6948,
      "step": 1775
    },
    {
      "epoch": 0.30352851903376754,
      "grad_norm": 0.6077538728713989,
      "learning_rate": 0.00016981100236247048,
      "loss": 0.6753,
      "step": 1800
    },
    {
      "epoch": 0.30774419290923655,
      "grad_norm": 0.42457517981529236,
      "learning_rate": 0.00016938913263584208,
      "loss": 0.5985,
      "step": 1825
    },
    {
      "epoch": 0.31195986678470555,
      "grad_norm": 0.5110180974006653,
      "learning_rate": 0.00016896726290921364,
      "loss": 0.5922,
      "step": 1850
    },
    {
      "epoch": 0.3161755406601745,
      "grad_norm": 0.6813841462135315,
      "learning_rate": 0.0001685453931825852,
      "loss": 0.6376,
      "step": 1875
    },
    {
      "epoch": 0.3203912145356435,
      "grad_norm": 0.41532841324806213,
      "learning_rate": 0.0001681235234559568,
      "loss": 0.6212,
      "step": 1900
    },
    {
      "epoch": 0.3246068884111125,
      "grad_norm": 0.4543830454349518,
      "learning_rate": 0.0001677016537293284,
      "loss": 0.5767,
      "step": 1925
    },
    {
      "epoch": 0.3288225622865815,
      "grad_norm": 0.38953709602355957,
      "learning_rate": 0.00016727978400269997,
      "loss": 0.7119,
      "step": 1950
    },
    {
      "epoch": 0.3330382361620505,
      "grad_norm": 0.6097630262374878,
      "learning_rate": 0.00016685791427607156,
      "loss": 0.6543,
      "step": 1975
    },
    {
      "epoch": 0.3372539100375195,
      "grad_norm": 0.42704203724861145,
      "learning_rate": 0.00016643604454944316,
      "loss": 0.6843,
      "step": 2000
    },
    {
      "epoch": 0.3414695839129885,
      "grad_norm": 0.4541128873825073,
      "learning_rate": 0.00016601417482281473,
      "loss": 0.6258,
      "step": 2025
    },
    {
      "epoch": 0.3456852577884575,
      "grad_norm": 0.6625218987464905,
      "learning_rate": 0.0001655923050961863,
      "loss": 0.6967,
      "step": 2050
    },
    {
      "epoch": 0.3499009316639265,
      "grad_norm": 0.6480218172073364,
      "learning_rate": 0.0001651704353695579,
      "loss": 0.5992,
      "step": 2075
    },
    {
      "epoch": 0.35411660553939545,
      "grad_norm": 0.5420606732368469,
      "learning_rate": 0.00016474856564292946,
      "loss": 0.6865,
      "step": 2100
    },
    {
      "epoch": 0.35833227941486445,
      "grad_norm": 0.5245200991630554,
      "learning_rate": 0.00016432669591630105,
      "loss": 0.6452,
      "step": 2125
    },
    {
      "epoch": 0.36254795329033346,
      "grad_norm": 0.40461981296539307,
      "learning_rate": 0.00016390482618967265,
      "loss": 0.6156,
      "step": 2150
    },
    {
      "epoch": 0.36676362716580246,
      "grad_norm": 0.48923876881599426,
      "learning_rate": 0.00016348295646304421,
      "loss": 0.589,
      "step": 2175
    },
    {
      "epoch": 0.37097930104127147,
      "grad_norm": 0.5824174284934998,
      "learning_rate": 0.0001630610867364158,
      "loss": 0.5449,
      "step": 2200
    },
    {
      "epoch": 0.3751949749167404,
      "grad_norm": 0.7029809355735779,
      "learning_rate": 0.0001626392170097874,
      "loss": 0.6917,
      "step": 2225
    },
    {
      "epoch": 0.3794106487922094,
      "grad_norm": 0.5921729803085327,
      "learning_rate": 0.00016221734728315897,
      "loss": 0.6561,
      "step": 2250
    },
    {
      "epoch": 0.38362632266767843,
      "grad_norm": 0.5674454569816589,
      "learning_rate": 0.00016179547755653054,
      "loss": 0.6677,
      "step": 2275
    },
    {
      "epoch": 0.38784199654314744,
      "grad_norm": 0.610201895236969,
      "learning_rate": 0.00016137360782990213,
      "loss": 0.5913,
      "step": 2300
    },
    {
      "epoch": 0.3920576704186164,
      "grad_norm": 0.5491762757301331,
      "learning_rate": 0.0001609517381032737,
      "loss": 0.5927,
      "step": 2325
    },
    {
      "epoch": 0.3962733442940854,
      "grad_norm": 0.3530021011829376,
      "learning_rate": 0.0001605298683766453,
      "loss": 0.6655,
      "step": 2350
    },
    {
      "epoch": 0.4004890181695544,
      "grad_norm": 0.7165349125862122,
      "learning_rate": 0.0001601079986500169,
      "loss": 0.6757,
      "step": 2375
    },
    {
      "epoch": 0.4047046920450234,
      "grad_norm": 0.6272157430648804,
      "learning_rate": 0.00015968612892338846,
      "loss": 0.6476,
      "step": 2400
    },
    {
      "epoch": 0.4089203659204924,
      "grad_norm": 0.47820591926574707,
      "learning_rate": 0.00015926425919676006,
      "loss": 0.6039,
      "step": 2425
    },
    {
      "epoch": 0.41313603979596136,
      "grad_norm": 0.6272923350334167,
      "learning_rate": 0.00015884238947013162,
      "loss": 0.5705,
      "step": 2450
    },
    {
      "epoch": 0.41735171367143037,
      "grad_norm": 0.6876711845397949,
      "learning_rate": 0.0001584205197435032,
      "loss": 0.6635,
      "step": 2475
    },
    {
      "epoch": 0.4215673875468994,
      "grad_norm": 0.5813876390457153,
      "learning_rate": 0.00015799865001687479,
      "loss": 0.6998,
      "step": 2500
    },
    {
      "epoch": 0.4257830614223684,
      "grad_norm": 0.5410722494125366,
      "learning_rate": 0.00015757678029024638,
      "loss": 0.6498,
      "step": 2525
    },
    {
      "epoch": 0.4299987352978374,
      "grad_norm": 0.4489375650882721,
      "learning_rate": 0.00015715491056361795,
      "loss": 0.6523,
      "step": 2550
    },
    {
      "epoch": 0.43421440917330634,
      "grad_norm": 0.32918059825897217,
      "learning_rate": 0.00015673304083698954,
      "loss": 0.5475,
      "step": 2575
    },
    {
      "epoch": 0.43843008304877534,
      "grad_norm": 0.6290420889854431,
      "learning_rate": 0.00015631117111036114,
      "loss": 0.5817,
      "step": 2600
    },
    {
      "epoch": 0.44264575692424435,
      "grad_norm": 0.5697997212409973,
      "learning_rate": 0.0001558893013837327,
      "loss": 0.6317,
      "step": 2625
    },
    {
      "epoch": 0.44686143079971336,
      "grad_norm": 0.47042548656463623,
      "learning_rate": 0.0001554674316571043,
      "loss": 0.6212,
      "step": 2650
    },
    {
      "epoch": 0.4510771046751823,
      "grad_norm": 0.4709092676639557,
      "learning_rate": 0.00015504556193047587,
      "loss": 0.6935,
      "step": 2675
    },
    {
      "epoch": 0.4552927785506513,
      "grad_norm": 0.43760597705841064,
      "learning_rate": 0.00015462369220384746,
      "loss": 0.5555,
      "step": 2700
    },
    {
      "epoch": 0.4595084524261203,
      "grad_norm": 0.6382158398628235,
      "learning_rate": 0.00015420182247721903,
      "loss": 0.665,
      "step": 2725
    },
    {
      "epoch": 0.4637241263015893,
      "grad_norm": 0.5693995952606201,
      "learning_rate": 0.00015377995275059063,
      "loss": 0.6546,
      "step": 2750
    },
    {
      "epoch": 0.46793980017705833,
      "grad_norm": 0.6461164355278015,
      "learning_rate": 0.00015335808302396222,
      "loss": 0.5976,
      "step": 2775
    },
    {
      "epoch": 0.4721554740525273,
      "grad_norm": 0.5676155090332031,
      "learning_rate": 0.0001529362132973338,
      "loss": 0.6376,
      "step": 2800
    },
    {
      "epoch": 0.4763711479279963,
      "grad_norm": 0.3362257480621338,
      "learning_rate": 0.00015251434357070538,
      "loss": 0.46,
      "step": 2825
    },
    {
      "epoch": 0.4805868218034653,
      "grad_norm": 0.4114580750465393,
      "learning_rate": 0.00015209247384407698,
      "loss": 0.6503,
      "step": 2850
    },
    {
      "epoch": 0.4848024956789343,
      "grad_norm": 0.37636545300483704,
      "learning_rate": 0.00015167060411744855,
      "loss": 0.6536,
      "step": 2875
    },
    {
      "epoch": 0.48901816955440325,
      "grad_norm": 0.7761818170547485,
      "learning_rate": 0.00015124873439082011,
      "loss": 0.6606,
      "step": 2900
    },
    {
      "epoch": 0.49323384342987225,
      "grad_norm": 0.541912853717804,
      "learning_rate": 0.0001508268646641917,
      "loss": 0.6287,
      "step": 2925
    },
    {
      "epoch": 0.49744951730534126,
      "grad_norm": 0.5690341591835022,
      "learning_rate": 0.00015040499493756328,
      "loss": 0.6333,
      "step": 2950
    },
    {
      "epoch": 0.5016651911808102,
      "grad_norm": 0.65323805809021,
      "learning_rate": 0.00014998312521093487,
      "loss": 0.6275,
      "step": 2975
    },
    {
      "epoch": 0.5058808650562793,
      "grad_norm": 0.6973729133605957,
      "learning_rate": 0.00014956125548430647,
      "loss": 0.6629,
      "step": 3000
    },
    {
      "epoch": 0.5100965389317482,
      "grad_norm": 0.5254755616188049,
      "learning_rate": 0.00014913938575767804,
      "loss": 0.7219,
      "step": 3025
    },
    {
      "epoch": 0.5143122128072173,
      "grad_norm": 0.6172437071800232,
      "learning_rate": 0.00014871751603104963,
      "loss": 0.6933,
      "step": 3050
    },
    {
      "epoch": 0.5185278866826862,
      "grad_norm": 0.3288564085960388,
      "learning_rate": 0.0001482956463044212,
      "loss": 0.573,
      "step": 3075
    },
    {
      "epoch": 0.5227435605581552,
      "grad_norm": 0.6276848316192627,
      "learning_rate": 0.00014787377657779277,
      "loss": 0.6586,
      "step": 3100
    },
    {
      "epoch": 0.5269592344336242,
      "grad_norm": 0.4938656687736511,
      "learning_rate": 0.00014745190685116436,
      "loss": 0.7353,
      "step": 3125
    },
    {
      "epoch": 0.5311749083090932,
      "grad_norm": 0.7912372946739197,
      "learning_rate": 0.00014703003712453596,
      "loss": 0.6351,
      "step": 3150
    },
    {
      "epoch": 0.5353905821845621,
      "grad_norm": 0.34613236784935,
      "learning_rate": 0.00014660816739790752,
      "loss": 0.7467,
      "step": 3175
    },
    {
      "epoch": 0.5396062560600312,
      "grad_norm": 0.6180621981620789,
      "learning_rate": 0.00014618629767127912,
      "loss": 0.5866,
      "step": 3200
    },
    {
      "epoch": 0.5438219299355002,
      "grad_norm": 0.3914062976837158,
      "learning_rate": 0.0001457644279446507,
      "loss": 0.5676,
      "step": 3225
    },
    {
      "epoch": 0.5480376038109692,
      "grad_norm": 0.5602525472640991,
      "learning_rate": 0.00014534255821802228,
      "loss": 0.5428,
      "step": 3250
    },
    {
      "epoch": 0.5522532776864382,
      "grad_norm": 0.5735276341438293,
      "learning_rate": 0.00014492068849139388,
      "loss": 0.6456,
      "step": 3275
    },
    {
      "epoch": 0.5564689515619071,
      "grad_norm": 0.34951359033584595,
      "learning_rate": 0.00014449881876476544,
      "loss": 0.5155,
      "step": 3300
    },
    {
      "epoch": 0.5606846254373762,
      "grad_norm": 0.6517199873924255,
      "learning_rate": 0.000144076949038137,
      "loss": 0.5248,
      "step": 3325
    },
    {
      "epoch": 0.5649002993128451,
      "grad_norm": 0.4959523677825928,
      "learning_rate": 0.0001436550793115086,
      "loss": 0.5785,
      "step": 3350
    },
    {
      "epoch": 0.5691159731883142,
      "grad_norm": 0.6868893504142761,
      "learning_rate": 0.0001432332095848802,
      "loss": 0.6247,
      "step": 3375
    },
    {
      "epoch": 0.5733316470637831,
      "grad_norm": 0.6509615182876587,
      "learning_rate": 0.00014281133985825177,
      "loss": 0.5059,
      "step": 3400
    },
    {
      "epoch": 0.5775473209392521,
      "grad_norm": 0.508507490158081,
      "learning_rate": 0.00014238947013162336,
      "loss": 0.6269,
      "step": 3425
    },
    {
      "epoch": 0.5817629948147212,
      "grad_norm": 0.4920637905597687,
      "learning_rate": 0.00014196760040499496,
      "loss": 0.6251,
      "step": 3450
    },
    {
      "epoch": 0.5859786686901901,
      "grad_norm": 0.2905634343624115,
      "learning_rate": 0.00014154573067836653,
      "loss": 0.5203,
      "step": 3475
    },
    {
      "epoch": 0.5901943425656592,
      "grad_norm": 0.6269543170928955,
      "learning_rate": 0.0001411238609517381,
      "loss": 0.6456,
      "step": 3500
    },
    {
      "epoch": 0.5944100164411281,
      "grad_norm": 0.5851781368255615,
      "learning_rate": 0.0001407019912251097,
      "loss": 0.5949,
      "step": 3525
    },
    {
      "epoch": 0.5986256903165971,
      "grad_norm": 0.5535221695899963,
      "learning_rate": 0.00014028012149848126,
      "loss": 0.5812,
      "step": 3550
    },
    {
      "epoch": 0.6028413641920661,
      "grad_norm": 0.6514847278594971,
      "learning_rate": 0.00013985825177185285,
      "loss": 0.5112,
      "step": 3575
    },
    {
      "epoch": 0.6070570380675351,
      "grad_norm": 0.6617153286933899,
      "learning_rate": 0.00013943638204522445,
      "loss": 0.5754,
      "step": 3600
    },
    {
      "epoch": 0.6112727119430041,
      "grad_norm": 0.5161499977111816,
      "learning_rate": 0.00013901451231859604,
      "loss": 0.5725,
      "step": 3625
    },
    {
      "epoch": 0.6154883858184731,
      "grad_norm": 0.6644999980926514,
      "learning_rate": 0.0001385926425919676,
      "loss": 0.6319,
      "step": 3650
    },
    {
      "epoch": 0.619704059693942,
      "grad_norm": 0.5403600931167603,
      "learning_rate": 0.0001381707728653392,
      "loss": 0.5136,
      "step": 3675
    },
    {
      "epoch": 0.6239197335694111,
      "grad_norm": 0.5741940140724182,
      "learning_rate": 0.00013774890313871077,
      "loss": 0.5649,
      "step": 3700
    },
    {
      "epoch": 0.6281354074448801,
      "grad_norm": 0.5928864479064941,
      "learning_rate": 0.00013732703341208234,
      "loss": 0.593,
      "step": 3725
    },
    {
      "epoch": 0.632351081320349,
      "grad_norm": 0.6705028414726257,
      "learning_rate": 0.00013690516368545394,
      "loss": 0.5893,
      "step": 3750
    },
    {
      "epoch": 0.6365667551958181,
      "grad_norm": 0.6181473731994629,
      "learning_rate": 0.00013648329395882553,
      "loss": 0.6296,
      "step": 3775
    },
    {
      "epoch": 0.640782429071287,
      "grad_norm": 0.2618084251880646,
      "learning_rate": 0.0001360614242321971,
      "loss": 0.6064,
      "step": 3800
    },
    {
      "epoch": 0.6449981029467561,
      "grad_norm": 0.7133391499519348,
      "learning_rate": 0.0001356395545055687,
      "loss": 0.5727,
      "step": 3825
    },
    {
      "epoch": 0.649213776822225,
      "grad_norm": 0.2838265001773834,
      "learning_rate": 0.0001352176847789403,
      "loss": 0.5874,
      "step": 3850
    },
    {
      "epoch": 0.653429450697694,
      "grad_norm": 0.6463882327079773,
      "learning_rate": 0.00013479581505231186,
      "loss": 0.6441,
      "step": 3875
    },
    {
      "epoch": 0.657645124573163,
      "grad_norm": 0.5738253593444824,
      "learning_rate": 0.00013437394532568342,
      "loss": 0.6203,
      "step": 3900
    },
    {
      "epoch": 0.661860798448632,
      "grad_norm": 0.6987646222114563,
      "learning_rate": 0.00013395207559905502,
      "loss": 0.6381,
      "step": 3925
    },
    {
      "epoch": 0.666076472324101,
      "grad_norm": 0.5635076761245728,
      "learning_rate": 0.00013353020587242659,
      "loss": 0.6958,
      "step": 3950
    },
    {
      "epoch": 0.67029214619957,
      "grad_norm": 0.4619535207748413,
      "learning_rate": 0.00013310833614579818,
      "loss": 0.5727,
      "step": 3975
    },
    {
      "epoch": 0.674507820075039,
      "grad_norm": 0.4986385107040405,
      "learning_rate": 0.00013268646641916978,
      "loss": 0.6043,
      "step": 4000
    },
    {
      "epoch": 0.678723493950508,
      "grad_norm": 0.5768849849700928,
      "learning_rate": 0.00013226459669254134,
      "loss": 0.5865,
      "step": 4025
    },
    {
      "epoch": 0.682939167825977,
      "grad_norm": 0.6660186052322388,
      "learning_rate": 0.00013184272696591294,
      "loss": 0.5481,
      "step": 4050
    },
    {
      "epoch": 0.687154841701446,
      "grad_norm": 0.6398460865020752,
      "learning_rate": 0.00013142085723928453,
      "loss": 0.6831,
      "step": 4075
    },
    {
      "epoch": 0.691370515576915,
      "grad_norm": 0.5294440388679504,
      "learning_rate": 0.0001309989875126561,
      "loss": 0.5794,
      "step": 4100
    },
    {
      "epoch": 0.6955861894523839,
      "grad_norm": 0.5679708123207092,
      "learning_rate": 0.00013057711778602767,
      "loss": 0.5743,
      "step": 4125
    },
    {
      "epoch": 0.699801863327853,
      "grad_norm": 0.6143545508384705,
      "learning_rate": 0.00013015524805939926,
      "loss": 0.5992,
      "step": 4150
    },
    {
      "epoch": 0.7040175372033219,
      "grad_norm": 0.5241633057594299,
      "learning_rate": 0.00012973337833277083,
      "loss": 0.631,
      "step": 4175
    },
    {
      "epoch": 0.7082332110787909,
      "grad_norm": 0.5035251975059509,
      "learning_rate": 0.00012931150860614243,
      "loss": 0.506,
      "step": 4200
    },
    {
      "epoch": 0.71244888495426,
      "grad_norm": 0.38072308897972107,
      "learning_rate": 0.00012888963887951402,
      "loss": 0.6068,
      "step": 4225
    },
    {
      "epoch": 0.7166645588297289,
      "grad_norm": 0.5499347448348999,
      "learning_rate": 0.0001284677691528856,
      "loss": 0.5837,
      "step": 4250
    },
    {
      "epoch": 0.720880232705198,
      "grad_norm": 0.48047101497650146,
      "learning_rate": 0.00012804589942625718,
      "loss": 0.5545,
      "step": 4275
    },
    {
      "epoch": 0.7250959065806669,
      "grad_norm": 0.4561833441257477,
      "learning_rate": 0.00012762402969962878,
      "loss": 0.6017,
      "step": 4300
    },
    {
      "epoch": 0.7293115804561359,
      "grad_norm": 0.467801958322525,
      "learning_rate": 0.00012720215997300035,
      "loss": 0.5628,
      "step": 4325
    },
    {
      "epoch": 0.7335272543316049,
      "grad_norm": 0.5959245562553406,
      "learning_rate": 0.00012678029024637192,
      "loss": 0.6,
      "step": 4350
    },
    {
      "epoch": 0.7377429282070739,
      "grad_norm": 0.7162978649139404,
      "learning_rate": 0.0001263584205197435,
      "loss": 0.6256,
      "step": 4375
    },
    {
      "epoch": 0.7419586020825429,
      "grad_norm": 0.3827458918094635,
      "learning_rate": 0.00012593655079311508,
      "loss": 0.5855,
      "step": 4400
    },
    {
      "epoch": 0.7461742759580119,
      "grad_norm": 0.6210541129112244,
      "learning_rate": 0.00012551468106648667,
      "loss": 0.7042,
      "step": 4425
    },
    {
      "epoch": 0.7503899498334808,
      "grad_norm": 0.7009960412979126,
      "learning_rate": 0.00012509281133985827,
      "loss": 0.675,
      "step": 4450
    },
    {
      "epoch": 0.7546056237089499,
      "grad_norm": 0.4742926359176636,
      "learning_rate": 0.00012467094161322984,
      "loss": 0.5351,
      "step": 4475
    },
    {
      "epoch": 0.7588212975844189,
      "grad_norm": 0.6871809959411621,
      "learning_rate": 0.00012424907188660143,
      "loss": 0.6094,
      "step": 4500
    },
    {
      "epoch": 0.7630369714598879,
      "grad_norm": 0.6019153594970703,
      "learning_rate": 0.000123827202159973,
      "loss": 0.5696,
      "step": 4525
    },
    {
      "epoch": 0.7672526453353569,
      "grad_norm": 0.5854367017745972,
      "learning_rate": 0.00012340533243334457,
      "loss": 0.596,
      "step": 4550
    },
    {
      "epoch": 0.7714683192108258,
      "grad_norm": 0.6882818937301636,
      "learning_rate": 0.00012298346270671616,
      "loss": 0.5454,
      "step": 4575
    },
    {
      "epoch": 0.7756839930862949,
      "grad_norm": 0.4133595824241638,
      "learning_rate": 0.00012256159298008776,
      "loss": 0.549,
      "step": 4600
    },
    {
      "epoch": 0.7798996669617638,
      "grad_norm": 0.6454863548278809,
      "learning_rate": 0.00012213972325345935,
      "loss": 0.6063,
      "step": 4625
    },
    {
      "epoch": 0.7841153408372328,
      "grad_norm": 0.7342830896377563,
      "learning_rate": 0.00012171785352683092,
      "loss": 0.5275,
      "step": 4650
    },
    {
      "epoch": 0.7883310147127018,
      "grad_norm": 0.6497771143913269,
      "learning_rate": 0.0001212959838002025,
      "loss": 0.6337,
      "step": 4675
    },
    {
      "epoch": 0.7925466885881708,
      "grad_norm": 0.4732552170753479,
      "learning_rate": 0.0001208741140735741,
      "loss": 0.5323,
      "step": 4700
    },
    {
      "epoch": 0.7967623624636399,
      "grad_norm": 0.5097313523292542,
      "learning_rate": 0.00012045224434694566,
      "loss": 0.5899,
      "step": 4725
    },
    {
      "epoch": 0.8009780363391088,
      "grad_norm": 0.4787701368331909,
      "learning_rate": 0.00012003037462031726,
      "loss": 0.5,
      "step": 4750
    },
    {
      "epoch": 0.8051937102145778,
      "grad_norm": 0.5041419267654419,
      "learning_rate": 0.00011960850489368884,
      "loss": 0.628,
      "step": 4775
    },
    {
      "epoch": 0.8094093840900468,
      "grad_norm": 0.6417216658592224,
      "learning_rate": 0.00011918663516706041,
      "loss": 0.6521,
      "step": 4800
    },
    {
      "epoch": 0.8136250579655158,
      "grad_norm": 0.5072162747383118,
      "learning_rate": 0.000118764765440432,
      "loss": 0.5122,
      "step": 4825
    },
    {
      "epoch": 0.8178407318409848,
      "grad_norm": 0.29049667716026306,
      "learning_rate": 0.0001183428957138036,
      "loss": 0.5814,
      "step": 4850
    },
    {
      "epoch": 0.8220564057164538,
      "grad_norm": 0.7524094581604004,
      "learning_rate": 0.00011792102598717516,
      "loss": 0.607,
      "step": 4875
    },
    {
      "epoch": 0.8262720795919227,
      "grad_norm": 0.537179708480835,
      "learning_rate": 0.00011749915626054675,
      "loss": 0.5462,
      "step": 4900
    },
    {
      "epoch": 0.8304877534673918,
      "grad_norm": 0.4518299698829651,
      "learning_rate": 0.00011707728653391834,
      "loss": 0.5757,
      "step": 4925
    },
    {
      "epoch": 0.8347034273428607,
      "grad_norm": 0.4717993438243866,
      "learning_rate": 0.00011665541680728991,
      "loss": 0.6238,
      "step": 4950
    },
    {
      "epoch": 0.8389191012183298,
      "grad_norm": 0.7028119564056396,
      "learning_rate": 0.0001162335470806615,
      "loss": 0.5341,
      "step": 4975
    },
    {
      "epoch": 0.8431347750937987,
      "grad_norm": 0.47149431705474854,
      "learning_rate": 0.00011581167735403309,
      "loss": 0.568,
      "step": 5000
    },
    {
      "epoch": 0.8473504489692677,
      "grad_norm": 0.5332790613174438,
      "learning_rate": 0.00011538980762740465,
      "loss": 0.5213,
      "step": 5025
    },
    {
      "epoch": 0.8515661228447368,
      "grad_norm": 0.6513734459877014,
      "learning_rate": 0.00011496793790077625,
      "loss": 0.6385,
      "step": 5050
    },
    {
      "epoch": 0.8557817967202057,
      "grad_norm": 0.48159608244895935,
      "learning_rate": 0.00011454606817414783,
      "loss": 0.5239,
      "step": 5075
    },
    {
      "epoch": 0.8599974705956748,
      "grad_norm": 0.6819217205047607,
      "learning_rate": 0.0001141241984475194,
      "loss": 0.6119,
      "step": 5100
    },
    {
      "epoch": 0.8642131444711437,
      "grad_norm": 0.703839123249054,
      "learning_rate": 0.00011370232872089099,
      "loss": 0.5921,
      "step": 5125
    },
    {
      "epoch": 0.8684288183466127,
      "grad_norm": 0.5300304293632507,
      "learning_rate": 0.00011328045899426259,
      "loss": 0.4969,
      "step": 5150
    },
    {
      "epoch": 0.8726444922220817,
      "grad_norm": 0.24977460503578186,
      "learning_rate": 0.00011285858926763415,
      "loss": 0.4973,
      "step": 5175
    },
    {
      "epoch": 0.8768601660975507,
      "grad_norm": 0.436964213848114,
      "learning_rate": 0.00011243671954100574,
      "loss": 0.574,
      "step": 5200
    },
    {
      "epoch": 0.8810758399730196,
      "grad_norm": 0.6595659255981445,
      "learning_rate": 0.00011201484981437733,
      "loss": 0.5918,
      "step": 5225
    },
    {
      "epoch": 0.8852915138484887,
      "grad_norm": 0.7338206171989441,
      "learning_rate": 0.0001115929800877489,
      "loss": 0.6525,
      "step": 5250
    },
    {
      "epoch": 0.8895071877239576,
      "grad_norm": 0.5962433218955994,
      "learning_rate": 0.0001111711103611205,
      "loss": 0.5804,
      "step": 5275
    },
    {
      "epoch": 0.8937228615994267,
      "grad_norm": 0.6299694180488586,
      "learning_rate": 0.00011074924063449208,
      "loss": 0.5338,
      "step": 5300
    },
    {
      "epoch": 0.8979385354748957,
      "grad_norm": 0.6968527436256409,
      "learning_rate": 0.00011032737090786364,
      "loss": 0.6213,
      "step": 5325
    },
    {
      "epoch": 0.9021542093503646,
      "grad_norm": 0.35897061228752136,
      "learning_rate": 0.00010990550118123524,
      "loss": 0.6021,
      "step": 5350
    },
    {
      "epoch": 0.9063698832258337,
      "grad_norm": 0.6376249194145203,
      "learning_rate": 0.00010948363145460683,
      "loss": 0.5814,
      "step": 5375
    },
    {
      "epoch": 0.9105855571013026,
      "grad_norm": 0.7169381976127625,
      "learning_rate": 0.0001090617617279784,
      "loss": 0.5293,
      "step": 5400
    },
    {
      "epoch": 0.9148012309767717,
      "grad_norm": 0.6142295598983765,
      "learning_rate": 0.00010863989200134998,
      "loss": 0.5357,
      "step": 5425
    },
    {
      "epoch": 0.9190169048522406,
      "grad_norm": 0.29530566930770874,
      "learning_rate": 0.00010821802227472158,
      "loss": 0.5452,
      "step": 5450
    },
    {
      "epoch": 0.9232325787277096,
      "grad_norm": 0.6458328366279602,
      "learning_rate": 0.00010779615254809314,
      "loss": 0.6358,
      "step": 5475
    },
    {
      "epoch": 0.9274482526031786,
      "grad_norm": 0.5853328108787537,
      "learning_rate": 0.00010737428282146474,
      "loss": 0.5302,
      "step": 5500
    },
    {
      "epoch": 0.9316639264786476,
      "grad_norm": 0.5087891221046448,
      "learning_rate": 0.00010695241309483632,
      "loss": 0.6497,
      "step": 5525
    },
    {
      "epoch": 0.9358796003541167,
      "grad_norm": 0.5413863658905029,
      "learning_rate": 0.00010653054336820792,
      "loss": 0.6033,
      "step": 5550
    },
    {
      "epoch": 0.9400952742295856,
      "grad_norm": 0.5840063095092773,
      "learning_rate": 0.00010610867364157948,
      "loss": 0.549,
      "step": 5575
    },
    {
      "epoch": 0.9443109481050546,
      "grad_norm": 0.46717432141304016,
      "learning_rate": 0.00010568680391495107,
      "loss": 0.6153,
      "step": 5600
    },
    {
      "epoch": 0.9485266219805236,
      "grad_norm": 0.37896203994750977,
      "learning_rate": 0.00010526493418832266,
      "loss": 0.595,
      "step": 5625
    },
    {
      "epoch": 0.9527422958559926,
      "grad_norm": 0.5025708079338074,
      "learning_rate": 0.00010484306446169423,
      "loss": 0.5781,
      "step": 5650
    },
    {
      "epoch": 0.9569579697314615,
      "grad_norm": 0.5325255990028381,
      "learning_rate": 0.00010442119473506582,
      "loss": 0.5653,
      "step": 5675
    },
    {
      "epoch": 0.9611736436069306,
      "grad_norm": 0.436511367559433,
      "learning_rate": 0.0001039993250084374,
      "loss": 0.5914,
      "step": 5700
    },
    {
      "epoch": 0.9653893174823995,
      "grad_norm": 0.6475377082824707,
      "learning_rate": 0.00010357745528180897,
      "loss": 0.567,
      "step": 5725
    },
    {
      "epoch": 0.9696049913578686,
      "grad_norm": 0.49628663063049316,
      "learning_rate": 0.00010315558555518057,
      "loss": 0.5908,
      "step": 5750
    },
    {
      "epoch": 0.9738206652333375,
      "grad_norm": 0.5009115934371948,
      "learning_rate": 0.00010273371582855216,
      "loss": 0.5096,
      "step": 5775
    },
    {
      "epoch": 0.9780363391088065,
      "grad_norm": 0.43752309679985046,
      "learning_rate": 0.00010231184610192373,
      "loss": 0.6214,
      "step": 5800
    },
    {
      "epoch": 0.9822520129842756,
      "grad_norm": 0.5446152091026306,
      "learning_rate": 0.00010188997637529531,
      "loss": 0.6389,
      "step": 5825
    },
    {
      "epoch": 0.9864676868597445,
      "grad_norm": 0.35858139395713806,
      "learning_rate": 0.0001014681066486669,
      "loss": 0.5722,
      "step": 5850
    },
    {
      "epoch": 0.9906833607352136,
      "grad_norm": 0.4427186846733093,
      "learning_rate": 0.00010104623692203847,
      "loss": 0.5296,
      "step": 5875
    },
    {
      "epoch": 0.9948990346106825,
      "grad_norm": 0.7008196711540222,
      "learning_rate": 0.00010062436719541007,
      "loss": 0.5847,
      "step": 5900
    },
    {
      "epoch": 0.9991147084861515,
      "grad_norm": 0.5215916633605957,
      "learning_rate": 0.00010020249746878165,
      "loss": 0.5625,
      "step": 5925
    },
    {
      "epoch": 1.0032039121453564,
      "grad_norm": 0.4655551612377167,
      "learning_rate": 9.978062774215323e-05,
      "loss": 0.4465,
      "step": 5950
    },
    {
      "epoch": 1.0074195860208255,
      "grad_norm": 0.6677742004394531,
      "learning_rate": 9.935875801552481e-05,
      "loss": 0.4998,
      "step": 5975
    },
    {
      "epoch": 1.0116352598962943,
      "grad_norm": 0.6689035892486572,
      "learning_rate": 9.89368882888964e-05,
      "loss": 0.522,
      "step": 6000
    },
    {
      "epoch": 1.0158509337717634,
      "grad_norm": 0.46287861466407776,
      "learning_rate": 9.851501856226798e-05,
      "loss": 0.4156,
      "step": 6025
    },
    {
      "epoch": 1.0200666076472324,
      "grad_norm": 0.4845119118690491,
      "learning_rate": 9.809314883563956e-05,
      "loss": 0.4534,
      "step": 6050
    },
    {
      "epoch": 1.0242822815227015,
      "grad_norm": 0.5268168449401855,
      "learning_rate": 9.767127910901114e-05,
      "loss": 0.5295,
      "step": 6075
    },
    {
      "epoch": 1.0284979553981703,
      "grad_norm": 0.2930879592895508,
      "learning_rate": 9.724940938238273e-05,
      "loss": 0.4678,
      "step": 6100
    },
    {
      "epoch": 1.0327136292736394,
      "grad_norm": 0.5559655427932739,
      "learning_rate": 9.68275396557543e-05,
      "loss": 0.5532,
      "step": 6125
    },
    {
      "epoch": 1.0369293031491085,
      "grad_norm": 0.713807225227356,
      "learning_rate": 9.640566992912588e-05,
      "loss": 0.4105,
      "step": 6150
    },
    {
      "epoch": 1.0411449770245773,
      "grad_norm": 0.710598349571228,
      "learning_rate": 9.598380020249748e-05,
      "loss": 0.5209,
      "step": 6175
    },
    {
      "epoch": 1.0453606509000464,
      "grad_norm": 0.6694005131721497,
      "learning_rate": 9.556193047586906e-05,
      "loss": 0.5114,
      "step": 6200
    },
    {
      "epoch": 1.0495763247755154,
      "grad_norm": 0.6505595445632935,
      "learning_rate": 9.514006074924064e-05,
      "loss": 0.5083,
      "step": 6225
    },
    {
      "epoch": 1.0537919986509843,
      "grad_norm": 0.5947447419166565,
      "learning_rate": 9.471819102261222e-05,
      "loss": 0.6027,
      "step": 6250
    },
    {
      "epoch": 1.0580076725264533,
      "grad_norm": 0.6662216782569885,
      "learning_rate": 9.42963212959838e-05,
      "loss": 0.4355,
      "step": 6275
    },
    {
      "epoch": 1.0622233464019224,
      "grad_norm": 0.9231926798820496,
      "learning_rate": 9.38744515693554e-05,
      "loss": 0.4925,
      "step": 6300
    },
    {
      "epoch": 1.0664390202773912,
      "grad_norm": 0.36717692017555237,
      "learning_rate": 9.345258184272697e-05,
      "loss": 0.5065,
      "step": 6325
    },
    {
      "epoch": 1.0706546941528603,
      "grad_norm": 0.8082122206687927,
      "learning_rate": 9.303071211609855e-05,
      "loss": 0.4571,
      "step": 6350
    },
    {
      "epoch": 1.0748703680283294,
      "grad_norm": 0.5144711136817932,
      "learning_rate": 9.260884238947014e-05,
      "loss": 0.5544,
      "step": 6375
    },
    {
      "epoch": 1.0790860419037984,
      "grad_norm": 0.7274363040924072,
      "learning_rate": 9.218697266284172e-05,
      "loss": 0.5245,
      "step": 6400
    },
    {
      "epoch": 1.0833017157792673,
      "grad_norm": 0.7450166940689087,
      "learning_rate": 9.17651029362133e-05,
      "loss": 0.4669,
      "step": 6425
    },
    {
      "epoch": 1.0875173896547363,
      "grad_norm": 0.7625418305397034,
      "learning_rate": 9.134323320958489e-05,
      "loss": 0.5538,
      "step": 6450
    },
    {
      "epoch": 1.0917330635302054,
      "grad_norm": 0.2771623432636261,
      "learning_rate": 9.092136348295647e-05,
      "loss": 0.5185,
      "step": 6475
    },
    {
      "epoch": 1.0959487374056742,
      "grad_norm": 0.5986016392707825,
      "learning_rate": 9.049949375632805e-05,
      "loss": 0.5001,
      "step": 6500
    },
    {
      "epoch": 1.1001644112811433,
      "grad_norm": 0.7495886087417603,
      "learning_rate": 9.007762402969963e-05,
      "loss": 0.5514,
      "step": 6525
    },
    {
      "epoch": 1.1043800851566123,
      "grad_norm": 0.5420746207237244,
      "learning_rate": 8.965575430307121e-05,
      "loss": 0.5246,
      "step": 6550
    },
    {
      "epoch": 1.1085957590320812,
      "grad_norm": 0.7179363369941711,
      "learning_rate": 8.923388457644279e-05,
      "loss": 0.4821,
      "step": 6575
    },
    {
      "epoch": 1.1128114329075502,
      "grad_norm": 0.6513673067092896,
      "learning_rate": 8.881201484981439e-05,
      "loss": 0.5086,
      "step": 6600
    },
    {
      "epoch": 1.1170271067830193,
      "grad_norm": 0.4887963533401489,
      "learning_rate": 8.839014512318597e-05,
      "loss": 0.4939,
      "step": 6625
    },
    {
      "epoch": 1.1212427806584881,
      "grad_norm": 0.25704720616340637,
      "learning_rate": 8.796827539655754e-05,
      "loss": 0.448,
      "step": 6650
    },
    {
      "epoch": 1.1254584545339572,
      "grad_norm": 0.5541170835494995,
      "learning_rate": 8.754640566992913e-05,
      "loss": 0.5323,
      "step": 6675
    },
    {
      "epoch": 1.1296741284094263,
      "grad_norm": 0.6498057842254639,
      "learning_rate": 8.712453594330071e-05,
      "loss": 0.5132,
      "step": 6700
    },
    {
      "epoch": 1.1338898022848953,
      "grad_norm": 0.7864496111869812,
      "learning_rate": 8.67026662166723e-05,
      "loss": 0.4741,
      "step": 6725
    },
    {
      "epoch": 1.1381054761603642,
      "grad_norm": 0.35012558102607727,
      "learning_rate": 8.628079649004388e-05,
      "loss": 0.5001,
      "step": 6750
    },
    {
      "epoch": 1.1423211500358332,
      "grad_norm": 0.6372642517089844,
      "learning_rate": 8.585892676341546e-05,
      "loss": 0.4813,
      "step": 6775
    },
    {
      "epoch": 1.1465368239113023,
      "grad_norm": 0.4799230992794037,
      "learning_rate": 8.543705703678705e-05,
      "loss": 0.4873,
      "step": 6800
    },
    {
      "epoch": 1.1507524977867711,
      "grad_norm": 0.6141244173049927,
      "learning_rate": 8.501518731015863e-05,
      "loss": 0.4605,
      "step": 6825
    },
    {
      "epoch": 1.1549681716622402,
      "grad_norm": 0.4965904653072357,
      "learning_rate": 8.45933175835302e-05,
      "loss": 0.5287,
      "step": 6850
    },
    {
      "epoch": 1.1591838455377093,
      "grad_norm": 0.4702877104282379,
      "learning_rate": 8.41714478569018e-05,
      "loss": 0.5808,
      "step": 6875
    },
    {
      "epoch": 1.1633995194131783,
      "grad_norm": 0.7296150922775269,
      "learning_rate": 8.374957813027338e-05,
      "loss": 0.5062,
      "step": 6900
    },
    {
      "epoch": 1.1676151932886472,
      "grad_norm": 0.7758309841156006,
      "learning_rate": 8.332770840364496e-05,
      "loss": 0.4337,
      "step": 6925
    },
    {
      "epoch": 1.1718308671641162,
      "grad_norm": 0.3434029221534729,
      "learning_rate": 8.290583867701654e-05,
      "loss": 0.4729,
      "step": 6950
    },
    {
      "epoch": 1.176046541039585,
      "grad_norm": 0.5509746074676514,
      "learning_rate": 8.248396895038812e-05,
      "loss": 0.574,
      "step": 6975
    },
    {
      "epoch": 1.1802622149150541,
      "grad_norm": 0.656366229057312,
      "learning_rate": 8.20620992237597e-05,
      "loss": 0.4839,
      "step": 7000
    },
    {
      "epoch": 1.1844778887905232,
      "grad_norm": 0.6406145095825195,
      "learning_rate": 8.16402294971313e-05,
      "loss": 0.5218,
      "step": 7025
    },
    {
      "epoch": 1.1886935626659922,
      "grad_norm": 0.6015857458114624,
      "learning_rate": 8.121835977050287e-05,
      "loss": 0.5016,
      "step": 7050
    },
    {
      "epoch": 1.192909236541461,
      "grad_norm": 0.747342050075531,
      "learning_rate": 8.079649004387445e-05,
      "loss": 0.5524,
      "step": 7075
    },
    {
      "epoch": 1.1971249104169301,
      "grad_norm": 0.36963751912117004,
      "learning_rate": 8.037462031724604e-05,
      "loss": 0.4756,
      "step": 7100
    },
    {
      "epoch": 1.2013405842923992,
      "grad_norm": 0.7366366386413574,
      "learning_rate": 7.995275059061762e-05,
      "loss": 0.4545,
      "step": 7125
    },
    {
      "epoch": 1.205556258167868,
      "grad_norm": 0.6472216844558716,
      "learning_rate": 7.95308808639892e-05,
      "loss": 0.4716,
      "step": 7150
    },
    {
      "epoch": 1.209771932043337,
      "grad_norm": 0.6171882152557373,
      "learning_rate": 7.910901113736079e-05,
      "loss": 0.5209,
      "step": 7175
    },
    {
      "epoch": 1.2139876059188062,
      "grad_norm": 0.5974865555763245,
      "learning_rate": 7.868714141073237e-05,
      "loss": 0.4829,
      "step": 7200
    },
    {
      "epoch": 1.2182032797942752,
      "grad_norm": 0.7058659195899963,
      "learning_rate": 7.826527168410396e-05,
      "loss": 0.5289,
      "step": 7225
    },
    {
      "epoch": 1.222418953669744,
      "grad_norm": 0.4835855960845947,
      "learning_rate": 7.784340195747553e-05,
      "loss": 0.4916,
      "step": 7250
    },
    {
      "epoch": 1.2266346275452131,
      "grad_norm": 0.47155624628067017,
      "learning_rate": 7.742153223084711e-05,
      "loss": 0.4524,
      "step": 7275
    },
    {
      "epoch": 1.2308503014206822,
      "grad_norm": 0.7369117736816406,
      "learning_rate": 7.69996625042187e-05,
      "loss": 0.5153,
      "step": 7300
    },
    {
      "epoch": 1.235065975296151,
      "grad_norm": 0.6700652837753296,
      "learning_rate": 7.657779277759029e-05,
      "loss": 0.5639,
      "step": 7325
    },
    {
      "epoch": 1.23928164917162,
      "grad_norm": 0.7910456657409668,
      "learning_rate": 7.615592305096187e-05,
      "loss": 0.502,
      "step": 7350
    },
    {
      "epoch": 1.2434973230470892,
      "grad_norm": 0.4138219654560089,
      "learning_rate": 7.573405332433345e-05,
      "loss": 0.4717,
      "step": 7375
    },
    {
      "epoch": 1.247712996922558,
      "grad_norm": 0.9390018582344055,
      "learning_rate": 7.531218359770503e-05,
      "loss": 0.5193,
      "step": 7400
    },
    {
      "epoch": 1.251928670798027,
      "grad_norm": 0.6529682278633118,
      "learning_rate": 7.489031387107661e-05,
      "loss": 0.4681,
      "step": 7425
    },
    {
      "epoch": 1.2561443446734961,
      "grad_norm": 0.3955245018005371,
      "learning_rate": 7.446844414444821e-05,
      "loss": 0.5093,
      "step": 7450
    },
    {
      "epoch": 1.260360018548965,
      "grad_norm": 0.7333526611328125,
      "learning_rate": 7.404657441781978e-05,
      "loss": 0.5822,
      "step": 7475
    },
    {
      "epoch": 1.264575692424434,
      "grad_norm": 0.7045482397079468,
      "learning_rate": 7.362470469119136e-05,
      "loss": 0.5041,
      "step": 7500
    },
    {
      "epoch": 1.268791366299903,
      "grad_norm": 0.612430989742279,
      "learning_rate": 7.320283496456295e-05,
      "loss": 0.4696,
      "step": 7525
    },
    {
      "epoch": 1.2730070401753721,
      "grad_norm": 0.7683935761451721,
      "learning_rate": 7.278096523793453e-05,
      "loss": 0.4713,
      "step": 7550
    },
    {
      "epoch": 1.277222714050841,
      "grad_norm": 0.5805240869522095,
      "learning_rate": 7.23590955113061e-05,
      "loss": 0.455,
      "step": 7575
    },
    {
      "epoch": 1.28143838792631,
      "grad_norm": 0.46375080943107605,
      "learning_rate": 7.19372257846777e-05,
      "loss": 0.4791,
      "step": 7600
    },
    {
      "epoch": 1.2856540618017789,
      "grad_norm": 0.7816604375839233,
      "learning_rate": 7.151535605804928e-05,
      "loss": 0.5046,
      "step": 7625
    },
    {
      "epoch": 1.289869735677248,
      "grad_norm": 0.6119601130485535,
      "learning_rate": 7.109348633142086e-05,
      "loss": 0.4817,
      "step": 7650
    },
    {
      "epoch": 1.294085409552717,
      "grad_norm": 0.4917443096637726,
      "learning_rate": 7.067161660479244e-05,
      "loss": 0.5045,
      "step": 7675
    },
    {
      "epoch": 1.298301083428186,
      "grad_norm": 0.8816726803779602,
      "learning_rate": 7.024974687816402e-05,
      "loss": 0.4811,
      "step": 7700
    },
    {
      "epoch": 1.3025167573036551,
      "grad_norm": 0.7599661350250244,
      "learning_rate": 6.982787715153562e-05,
      "loss": 0.5441,
      "step": 7725
    },
    {
      "epoch": 1.306732431179124,
      "grad_norm": 0.7378782033920288,
      "learning_rate": 6.94060074249072e-05,
      "loss": 0.5135,
      "step": 7750
    },
    {
      "epoch": 1.310948105054593,
      "grad_norm": 0.881053626537323,
      "learning_rate": 6.898413769827877e-05,
      "loss": 0.4622,
      "step": 7775
    },
    {
      "epoch": 1.3151637789300619,
      "grad_norm": 0.7380149364471436,
      "learning_rate": 6.856226797165036e-05,
      "loss": 0.454,
      "step": 7800
    },
    {
      "epoch": 1.319379452805531,
      "grad_norm": 0.7978062629699707,
      "learning_rate": 6.814039824502194e-05,
      "loss": 0.5204,
      "step": 7825
    },
    {
      "epoch": 1.323595126681,
      "grad_norm": 0.7326875329017639,
      "learning_rate": 6.771852851839352e-05,
      "loss": 0.4773,
      "step": 7850
    },
    {
      "epoch": 1.327810800556469,
      "grad_norm": 0.40200042724609375,
      "learning_rate": 6.72966587917651e-05,
      "loss": 0.4628,
      "step": 7875
    },
    {
      "epoch": 1.332026474431938,
      "grad_norm": 0.7688040733337402,
      "learning_rate": 6.687478906513669e-05,
      "loss": 0.4726,
      "step": 7900
    },
    {
      "epoch": 1.336242148307407,
      "grad_norm": 0.8773890733718872,
      "learning_rate": 6.645291933850827e-05,
      "loss": 0.5036,
      "step": 7925
    },
    {
      "epoch": 1.340457822182876,
      "grad_norm": 0.2671263515949249,
      "learning_rate": 6.603104961187986e-05,
      "loss": 0.514,
      "step": 7950
    },
    {
      "epoch": 1.3446734960583449,
      "grad_norm": 0.865651547908783,
      "learning_rate": 6.560917988525143e-05,
      "loss": 0.4978,
      "step": 7975
    },
    {
      "epoch": 1.348889169933814,
      "grad_norm": 0.6994335055351257,
      "learning_rate": 6.518731015862301e-05,
      "loss": 0.4387,
      "step": 8000
    },
    {
      "epoch": 1.353104843809283,
      "grad_norm": 0.8716712594032288,
      "learning_rate": 6.476544043199461e-05,
      "loss": 0.4829,
      "step": 8025
    },
    {
      "epoch": 1.357320517684752,
      "grad_norm": 0.7104661464691162,
      "learning_rate": 6.434357070536619e-05,
      "loss": 0.5289,
      "step": 8050
    },
    {
      "epoch": 1.3615361915602209,
      "grad_norm": 0.7121756672859192,
      "learning_rate": 6.392170097873777e-05,
      "loss": 0.4987,
      "step": 8075
    },
    {
      "epoch": 1.36575186543569,
      "grad_norm": 0.6892464756965637,
      "learning_rate": 6.349983125210935e-05,
      "loss": 0.5015,
      "step": 8100
    },
    {
      "epoch": 1.3699675393111588,
      "grad_norm": 0.7717614769935608,
      "learning_rate": 6.307796152548093e-05,
      "loss": 0.5049,
      "step": 8125
    },
    {
      "epoch": 1.3741832131866278,
      "grad_norm": 0.7690176963806152,
      "learning_rate": 6.265609179885251e-05,
      "loss": 0.4783,
      "step": 8150
    },
    {
      "epoch": 1.378398887062097,
      "grad_norm": 0.6564702987670898,
      "learning_rate": 6.223422207222411e-05,
      "loss": 0.5235,
      "step": 8175
    },
    {
      "epoch": 1.382614560937566,
      "grad_norm": 0.7037317156791687,
      "learning_rate": 6.181235234559568e-05,
      "loss": 0.4655,
      "step": 8200
    },
    {
      "epoch": 1.3868302348130348,
      "grad_norm": 0.7554595470428467,
      "learning_rate": 6.139048261896727e-05,
      "loss": 0.4245,
      "step": 8225
    },
    {
      "epoch": 1.3910459086885039,
      "grad_norm": 0.7984418869018555,
      "learning_rate": 6.096861289233885e-05,
      "loss": 0.4861,
      "step": 8250
    },
    {
      "epoch": 1.395261582563973,
      "grad_norm": 0.615343451499939,
      "learning_rate": 6.054674316571043e-05,
      "loss": 0.5149,
      "step": 8275
    },
    {
      "epoch": 1.3994772564394418,
      "grad_norm": 0.7946416139602661,
      "learning_rate": 6.0124873439082016e-05,
      "loss": 0.4904,
      "step": 8300
    },
    {
      "epoch": 1.4036929303149108,
      "grad_norm": 0.601699948310852,
      "learning_rate": 5.97030037124536e-05,
      "loss": 0.4531,
      "step": 8325
    },
    {
      "epoch": 1.4079086041903799,
      "grad_norm": 0.6090298891067505,
      "learning_rate": 5.928113398582518e-05,
      "loss": 0.5591,
      "step": 8350
    },
    {
      "epoch": 1.412124278065849,
      "grad_norm": 0.6948973536491394,
      "learning_rate": 5.8859264259196767e-05,
      "loss": 0.5867,
      "step": 8375
    },
    {
      "epoch": 1.4163399519413178,
      "grad_norm": 0.24949559569358826,
      "learning_rate": 5.843739453256835e-05,
      "loss": 0.4815,
      "step": 8400
    },
    {
      "epoch": 1.4205556258167868,
      "grad_norm": 0.30321982502937317,
      "learning_rate": 5.801552480593992e-05,
      "loss": 0.4805,
      "step": 8425
    },
    {
      "epoch": 1.4247712996922557,
      "grad_norm": 0.6853616237640381,
      "learning_rate": 5.759365507931152e-05,
      "loss": 0.4285,
      "step": 8450
    },
    {
      "epoch": 1.4289869735677247,
      "grad_norm": 0.8004145622253418,
      "learning_rate": 5.717178535268309e-05,
      "loss": 0.498,
      "step": 8475
    },
    {
      "epoch": 1.4332026474431938,
      "grad_norm": 0.7599137425422668,
      "learning_rate": 5.674991562605467e-05,
      "loss": 0.4517,
      "step": 8500
    },
    {
      "epoch": 1.4374183213186629,
      "grad_norm": 0.7603192925453186,
      "learning_rate": 5.632804589942626e-05,
      "loss": 0.4157,
      "step": 8525
    },
    {
      "epoch": 1.4416339951941317,
      "grad_norm": 0.7910857796669006,
      "learning_rate": 5.590617617279784e-05,
      "loss": 0.5158,
      "step": 8550
    },
    {
      "epoch": 1.4458496690696008,
      "grad_norm": 0.7243565320968628,
      "learning_rate": 5.548430644616942e-05,
      "loss": 0.5402,
      "step": 8575
    },
    {
      "epoch": 1.4500653429450698,
      "grad_norm": 0.3747308552265167,
      "learning_rate": 5.506243671954101e-05,
      "loss": 0.4719,
      "step": 8600
    },
    {
      "epoch": 1.4542810168205387,
      "grad_norm": 0.6008052229881287,
      "learning_rate": 5.464056699291259e-05,
      "loss": 0.4946,
      "step": 8625
    },
    {
      "epoch": 1.4584966906960077,
      "grad_norm": 0.7172877192497253,
      "learning_rate": 5.421869726628417e-05,
      "loss": 0.48,
      "step": 8650
    },
    {
      "epoch": 1.4627123645714768,
      "grad_norm": 0.7754296660423279,
      "learning_rate": 5.3796827539655757e-05,
      "loss": 0.4425,
      "step": 8675
    },
    {
      "epoch": 1.4669280384469459,
      "grad_norm": 0.7708630561828613,
      "learning_rate": 5.337495781302734e-05,
      "loss": 0.4398,
      "step": 8700
    },
    {
      "epoch": 1.4711437123224147,
      "grad_norm": 0.8373287320137024,
      "learning_rate": 5.2953088086398926e-05,
      "loss": 0.4526,
      "step": 8725
    },
    {
      "epoch": 1.4753593861978838,
      "grad_norm": 0.7733048796653748,
      "learning_rate": 5.253121835977051e-05,
      "loss": 0.4514,
      "step": 8750
    },
    {
      "epoch": 1.4795750600733526,
      "grad_norm": 0.6870519518852234,
      "learning_rate": 5.210934863314209e-05,
      "loss": 0.4794,
      "step": 8775
    },
    {
      "epoch": 1.4837907339488217,
      "grad_norm": 0.7710301280021667,
      "learning_rate": 5.168747890651368e-05,
      "loss": 0.5127,
      "step": 8800
    },
    {
      "epoch": 1.4880064078242907,
      "grad_norm": 0.5956792235374451,
      "learning_rate": 5.126560917988525e-05,
      "loss": 0.5086,
      "step": 8825
    },
    {
      "epoch": 1.4922220816997598,
      "grad_norm": 0.7209194302558899,
      "learning_rate": 5.084373945325683e-05,
      "loss": 0.4859,
      "step": 8850
    },
    {
      "epoch": 1.4964377555752286,
      "grad_norm": 0.8648145794868469,
      "learning_rate": 5.042186972662842e-05,
      "loss": 0.4323,
      "step": 8875
    },
    {
      "epoch": 1.5006534294506977,
      "grad_norm": 0.9173420667648315,
      "learning_rate": 5e-05,
      "loss": 0.5469,
      "step": 8900
    },
    {
      "epoch": 1.5048691033261665,
      "grad_norm": 0.6567739248275757,
      "learning_rate": 4.9578130273371584e-05,
      "loss": 0.4693,
      "step": 8925
    },
    {
      "epoch": 1.5090847772016356,
      "grad_norm": 0.7825670838356018,
      "learning_rate": 4.9156260546743165e-05,
      "loss": 0.5229,
      "step": 8950
    },
    {
      "epoch": 1.5133004510771046,
      "grad_norm": 0.2446974515914917,
      "learning_rate": 4.873439082011475e-05,
      "loss": 0.5543,
      "step": 8975
    },
    {
      "epoch": 1.5175161249525737,
      "grad_norm": 0.7899777889251709,
      "learning_rate": 4.8312521093486335e-05,
      "loss": 0.5104,
      "step": 9000
    },
    {
      "epoch": 1.5217317988280428,
      "grad_norm": 0.9442755579948425,
      "learning_rate": 4.7890651366857916e-05,
      "loss": 0.5374,
      "step": 9025
    },
    {
      "epoch": 1.5259474727035116,
      "grad_norm": 0.3034554719924927,
      "learning_rate": 4.74687816402295e-05,
      "loss": 0.5115,
      "step": 9050
    },
    {
      "epoch": 1.5301631465789807,
      "grad_norm": 0.4855184555053711,
      "learning_rate": 4.7046911913601086e-05,
      "loss": 0.4569,
      "step": 9075
    },
    {
      "epoch": 1.5343788204544495,
      "grad_norm": 0.48334819078445435,
      "learning_rate": 4.662504218697267e-05,
      "loss": 0.4456,
      "step": 9100
    },
    {
      "epoch": 1.5385944943299186,
      "grad_norm": 0.25468945503234863,
      "learning_rate": 4.620317246034425e-05,
      "loss": 0.4941,
      "step": 9125
    },
    {
      "epoch": 1.5428101682053876,
      "grad_norm": 0.8908275961875916,
      "learning_rate": 4.578130273371583e-05,
      "loss": 0.5036,
      "step": 9150
    },
    {
      "epoch": 1.5470258420808567,
      "grad_norm": 0.32419005036354065,
      "learning_rate": 4.535943300708742e-05,
      "loss": 0.4461,
      "step": 9175
    },
    {
      "epoch": 1.5512415159563258,
      "grad_norm": 0.7747356295585632,
      "learning_rate": 4.493756328045899e-05,
      "loss": 0.4969,
      "step": 9200
    },
    {
      "epoch": 1.5554571898317946,
      "grad_norm": 0.6816151738166809,
      "learning_rate": 4.451569355383058e-05,
      "loss": 0.4946,
      "step": 9225
    },
    {
      "epoch": 1.5596728637072637,
      "grad_norm": 0.7619186043739319,
      "learning_rate": 4.409382382720216e-05,
      "loss": 0.4828,
      "step": 9250
    },
    {
      "epoch": 1.5638885375827325,
      "grad_norm": 0.4772481918334961,
      "learning_rate": 4.367195410057375e-05,
      "loss": 0.4765,
      "step": 9275
    },
    {
      "epoch": 1.5681042114582016,
      "grad_norm": 0.9339179396629333,
      "learning_rate": 4.3250084373945325e-05,
      "loss": 0.5602,
      "step": 9300
    },
    {
      "epoch": 1.5723198853336706,
      "grad_norm": 0.8074682354927063,
      "learning_rate": 4.282821464731691e-05,
      "loss": 0.5236,
      "step": 9325
    },
    {
      "epoch": 1.5765355592091397,
      "grad_norm": 0.5138136744499207,
      "learning_rate": 4.2406344920688494e-05,
      "loss": 0.467,
      "step": 9350
    },
    {
      "epoch": 1.5807512330846085,
      "grad_norm": 0.7830900549888611,
      "learning_rate": 4.1984475194060076e-05,
      "loss": 0.4885,
      "step": 9375
    },
    {
      "epoch": 1.5849669069600776,
      "grad_norm": 0.8853349089622498,
      "learning_rate": 4.156260546743166e-05,
      "loss": 0.4268,
      "step": 9400
    },
    {
      "epoch": 1.5891825808355464,
      "grad_norm": 0.8798447847366333,
      "learning_rate": 4.1140735740803245e-05,
      "loss": 0.4879,
      "step": 9425
    },
    {
      "epoch": 1.5933982547110155,
      "grad_norm": 0.8938064575195312,
      "learning_rate": 4.071886601417482e-05,
      "loss": 0.4493,
      "step": 9450
    },
    {
      "epoch": 1.5976139285864845,
      "grad_norm": 0.8983244299888611,
      "learning_rate": 4.029699628754641e-05,
      "loss": 0.5057,
      "step": 9475
    },
    {
      "epoch": 1.6018296024619536,
      "grad_norm": 0.6637875437736511,
      "learning_rate": 3.987512656091799e-05,
      "loss": 0.514,
      "step": 9500
    },
    {
      "epoch": 1.6060452763374227,
      "grad_norm": 0.5936872363090515,
      "learning_rate": 3.945325683428958e-05,
      "loss": 0.4219,
      "step": 9525
    },
    {
      "epoch": 1.6102609502128915,
      "grad_norm": 0.5511206984519958,
      "learning_rate": 3.903138710766115e-05,
      "loss": 0.471,
      "step": 9550
    },
    {
      "epoch": 1.6144766240883606,
      "grad_norm": 0.6283127069473267,
      "learning_rate": 3.860951738103274e-05,
      "loss": 0.4345,
      "step": 9575
    },
    {
      "epoch": 1.6186922979638294,
      "grad_norm": 0.6517462134361267,
      "learning_rate": 3.818764765440432e-05,
      "loss": 0.4487,
      "step": 9600
    },
    {
      "epoch": 1.6229079718392985,
      "grad_norm": 0.4480813443660736,
      "learning_rate": 3.77657779277759e-05,
      "loss": 0.477,
      "step": 9625
    },
    {
      "epoch": 1.6271236457147675,
      "grad_norm": 0.38295117020606995,
      "learning_rate": 3.7343908201147484e-05,
      "loss": 0.462,
      "step": 9650
    },
    {
      "epoch": 1.6313393195902366,
      "grad_norm": 0.555801272392273,
      "learning_rate": 3.692203847451907e-05,
      "loss": 0.4708,
      "step": 9675
    },
    {
      "epoch": 1.6355549934657057,
      "grad_norm": 0.3055196702480316,
      "learning_rate": 3.6500168747890654e-05,
      "loss": 0.3987,
      "step": 9700
    },
    {
      "epoch": 1.6397706673411745,
      "grad_norm": 0.39763563871383667,
      "learning_rate": 3.6078299021262235e-05,
      "loss": 0.4696,
      "step": 9725
    },
    {
      "epoch": 1.6439863412166433,
      "grad_norm": 0.5356262922286987,
      "learning_rate": 3.5656429294633816e-05,
      "loss": 0.4633,
      "step": 9750
    },
    {
      "epoch": 1.6482020150921124,
      "grad_norm": 0.7265201807022095,
      "learning_rate": 3.5234559568005405e-05,
      "loss": 0.4991,
      "step": 9775
    },
    {
      "epoch": 1.6524176889675815,
      "grad_norm": 0.8920220136642456,
      "learning_rate": 3.4812689841376986e-05,
      "loss": 0.5408,
      "step": 9800
    },
    {
      "epoch": 1.6566333628430505,
      "grad_norm": 0.250211626291275,
      "learning_rate": 3.439082011474857e-05,
      "loss": 0.4698,
      "step": 9825
    },
    {
      "epoch": 1.6608490367185196,
      "grad_norm": 0.41475674510002136,
      "learning_rate": 3.3968950388120155e-05,
      "loss": 0.4774,
      "step": 9850
    },
    {
      "epoch": 1.6650647105939884,
      "grad_norm": 0.6278113126754761,
      "learning_rate": 3.354708066149173e-05,
      "loss": 0.4947,
      "step": 9875
    },
    {
      "epoch": 1.6692803844694575,
      "grad_norm": 0.7429221272468567,
      "learning_rate": 3.312521093486332e-05,
      "loss": 0.503,
      "step": 9900
    },
    {
      "epoch": 1.6734960583449263,
      "grad_norm": 0.6528053879737854,
      "learning_rate": 3.27033412082349e-05,
      "loss": 0.5029,
      "step": 9925
    },
    {
      "epoch": 1.6777117322203954,
      "grad_norm": 0.5085238814353943,
      "learning_rate": 3.228147148160648e-05,
      "loss": 0.4916,
      "step": 9950
    },
    {
      "epoch": 1.6819274060958644,
      "grad_norm": 0.8245604634284973,
      "learning_rate": 3.185960175497806e-05,
      "loss": 0.5486,
      "step": 9975
    },
    {
      "epoch": 1.6861430799713335,
      "grad_norm": 0.732822835445404,
      "learning_rate": 3.143773202834965e-05,
      "loss": 0.4571,
      "step": 10000
    },
    {
      "epoch": 1.6903587538468026,
      "grad_norm": 0.6564021706581116,
      "learning_rate": 3.101586230172123e-05,
      "loss": 0.4788,
      "step": 10025
    },
    {
      "epoch": 1.6945744277222714,
      "grad_norm": 0.7690842747688293,
      "learning_rate": 3.059399257509281e-05,
      "loss": 0.4489,
      "step": 10050
    },
    {
      "epoch": 1.6987901015977402,
      "grad_norm": 0.6155146956443787,
      "learning_rate": 3.0172122848464395e-05,
      "loss": 0.4871,
      "step": 10075
    },
    {
      "epoch": 1.7030057754732093,
      "grad_norm": 0.5544552803039551,
      "learning_rate": 2.975025312183598e-05,
      "loss": 0.4559,
      "step": 10100
    },
    {
      "epoch": 1.7072214493486784,
      "grad_norm": 0.7735791206359863,
      "learning_rate": 2.932838339520756e-05,
      "loss": 0.4667,
      "step": 10125
    },
    {
      "epoch": 1.7114371232241474,
      "grad_norm": 0.7303581237792969,
      "learning_rate": 2.8906513668579145e-05,
      "loss": 0.4996,
      "step": 10150
    },
    {
      "epoch": 1.7156527970996165,
      "grad_norm": 0.8121477961540222,
      "learning_rate": 2.8484643941950727e-05,
      "loss": 0.4978,
      "step": 10175
    },
    {
      "epoch": 1.7198684709750853,
      "grad_norm": 0.649330735206604,
      "learning_rate": 2.8062774215322308e-05,
      "loss": 0.4725,
      "step": 10200
    },
    {
      "epoch": 1.7240841448505544,
      "grad_norm": 0.7154254913330078,
      "learning_rate": 2.7640904488693893e-05,
      "loss": 0.4899,
      "step": 10225
    },
    {
      "epoch": 1.7282998187260232,
      "grad_norm": 0.46379855275154114,
      "learning_rate": 2.7219034762065478e-05,
      "loss": 0.4148,
      "step": 10250
    },
    {
      "epoch": 1.7325154926014923,
      "grad_norm": 0.6467633247375488,
      "learning_rate": 2.679716503543706e-05,
      "loss": 0.4346,
      "step": 10275
    },
    {
      "epoch": 1.7367311664769614,
      "grad_norm": 0.6238073706626892,
      "learning_rate": 2.637529530880864e-05,
      "loss": 0.4259,
      "step": 10300
    },
    {
      "epoch": 1.7409468403524304,
      "grad_norm": 0.7978204488754272,
      "learning_rate": 2.5953425582180225e-05,
      "loss": 0.4255,
      "step": 10325
    },
    {
      "epoch": 1.7451625142278995,
      "grad_norm": 0.6016398072242737,
      "learning_rate": 2.553155585555181e-05,
      "loss": 0.556,
      "step": 10350
    },
    {
      "epoch": 1.7493781881033683,
      "grad_norm": 0.6592808365821838,
      "learning_rate": 2.5109686128923388e-05,
      "loss": 0.4796,
      "step": 10375
    },
    {
      "epoch": 1.7535938619788372,
      "grad_norm": 0.9504634737968445,
      "learning_rate": 2.4687816402294973e-05,
      "loss": 0.4615,
      "step": 10400
    },
    {
      "epoch": 1.7578095358543062,
      "grad_norm": 0.6282676458358765,
      "learning_rate": 2.4265946675666557e-05,
      "loss": 0.5617,
      "step": 10425
    },
    {
      "epoch": 1.7620252097297753,
      "grad_norm": 0.5506725907325745,
      "learning_rate": 2.384407694903814e-05,
      "loss": 0.5267,
      "step": 10450
    },
    {
      "epoch": 1.7662408836052443,
      "grad_norm": 0.7033785581588745,
      "learning_rate": 2.342220722240972e-05,
      "loss": 0.4822,
      "step": 10475
    },
    {
      "epoch": 1.7704565574807134,
      "grad_norm": 0.5643253922462463,
      "learning_rate": 2.3000337495781305e-05,
      "loss": 0.4955,
      "step": 10500
    },
    {
      "epoch": 1.7746722313561822,
      "grad_norm": 0.6547824144363403,
      "learning_rate": 2.2578467769152886e-05,
      "loss": 0.5043,
      "step": 10525
    },
    {
      "epoch": 1.7788879052316513,
      "grad_norm": 0.5584544539451599,
      "learning_rate": 2.215659804252447e-05,
      "loss": 0.4887,
      "step": 10550
    },
    {
      "epoch": 1.7831035791071201,
      "grad_norm": 0.25532782077789307,
      "learning_rate": 2.1734728315896052e-05,
      "loss": 0.4412,
      "step": 10575
    },
    {
      "epoch": 1.7873192529825892,
      "grad_norm": 0.6661017537117004,
      "learning_rate": 2.1312858589267634e-05,
      "loss": 0.439,
      "step": 10600
    },
    {
      "epoch": 1.7915349268580583,
      "grad_norm": 0.673974335193634,
      "learning_rate": 2.089098886263922e-05,
      "loss": 0.4028,
      "step": 10625
    },
    {
      "epoch": 1.7957506007335273,
      "grad_norm": 0.8411164879798889,
      "learning_rate": 2.04691191360108e-05,
      "loss": 0.5883,
      "step": 10650
    },
    {
      "epoch": 1.7999662746089964,
      "grad_norm": 0.786247730255127,
      "learning_rate": 2.0047249409382385e-05,
      "loss": 0.4745,
      "step": 10675
    },
    {
      "epoch": 1.8041819484844652,
      "grad_norm": 0.8517912030220032,
      "learning_rate": 1.9625379682753966e-05,
      "loss": 0.4816,
      "step": 10700
    },
    {
      "epoch": 1.8083976223599343,
      "grad_norm": 0.833970844745636,
      "learning_rate": 1.9203509956125547e-05,
      "loss": 0.4633,
      "step": 10725
    },
    {
      "epoch": 1.8126132962354031,
      "grad_norm": 0.84242844581604,
      "learning_rate": 1.8781640229497132e-05,
      "loss": 0.5218,
      "step": 10750
    },
    {
      "epoch": 1.8168289701108722,
      "grad_norm": 0.7820282578468323,
      "learning_rate": 1.8359770502868713e-05,
      "loss": 0.5319,
      "step": 10775
    },
    {
      "epoch": 1.8210446439863412,
      "grad_norm": 0.616072952747345,
      "learning_rate": 1.7937900776240298e-05,
      "loss": 0.4789,
      "step": 10800
    },
    {
      "epoch": 1.8252603178618103,
      "grad_norm": 0.6939791440963745,
      "learning_rate": 1.751603104961188e-05,
      "loss": 0.4441,
      "step": 10825
    },
    {
      "epoch": 1.8294759917372792,
      "grad_norm": 0.7682216763496399,
      "learning_rate": 1.709416132298346e-05,
      "loss": 0.4974,
      "step": 10850
    },
    {
      "epoch": 1.8336916656127482,
      "grad_norm": 0.931611180305481,
      "learning_rate": 1.6672291596355046e-05,
      "loss": 0.5326,
      "step": 10875
    },
    {
      "epoch": 1.837907339488217,
      "grad_norm": 0.26045799255371094,
      "learning_rate": 1.6250421869726627e-05,
      "loss": 0.3897,
      "step": 10900
    },
    {
      "epoch": 1.8421230133636861,
      "grad_norm": 0.5416908860206604,
      "learning_rate": 1.5828552143098212e-05,
      "loss": 0.4659,
      "step": 10925
    },
    {
      "epoch": 1.8463386872391552,
      "grad_norm": 0.7700273990631104,
      "learning_rate": 1.5406682416469793e-05,
      "loss": 0.5174,
      "step": 10950
    },
    {
      "epoch": 1.8505543611146242,
      "grad_norm": 0.6734827160835266,
      "learning_rate": 1.4984812689841376e-05,
      "loss": 0.4537,
      "step": 10975
    },
    {
      "epoch": 1.8547700349900933,
      "grad_norm": 0.34074831008911133,
      "learning_rate": 1.4562942963212961e-05,
      "loss": 0.5224,
      "step": 11000
    },
    {
      "epoch": 1.8589857088655621,
      "grad_norm": 0.9279959797859192,
      "learning_rate": 1.4141073236584542e-05,
      "loss": 0.5053,
      "step": 11025
    },
    {
      "epoch": 1.8632013827410312,
      "grad_norm": 0.5429545640945435,
      "learning_rate": 1.3719203509956127e-05,
      "loss": 0.4386,
      "step": 11050
    },
    {
      "epoch": 1.8674170566165,
      "grad_norm": 0.5422692894935608,
      "learning_rate": 1.3297333783327709e-05,
      "loss": 0.5139,
      "step": 11075
    },
    {
      "epoch": 1.871632730491969,
      "grad_norm": 0.7585718631744385,
      "learning_rate": 1.287546405669929e-05,
      "loss": 0.5144,
      "step": 11100
    },
    {
      "epoch": 1.8758484043674382,
      "grad_norm": 1.1645915508270264,
      "learning_rate": 1.2453594330070875e-05,
      "loss": 0.4459,
      "step": 11125
    },
    {
      "epoch": 1.8800640782429072,
      "grad_norm": 0.4664171636104584,
      "learning_rate": 1.2031724603442456e-05,
      "loss": 0.5268,
      "step": 11150
    },
    {
      "epoch": 1.8842797521183763,
      "grad_norm": 0.7786681652069092,
      "learning_rate": 1.160985487681404e-05,
      "loss": 0.5355,
      "step": 11175
    },
    {
      "epoch": 1.8884954259938451,
      "grad_norm": 0.7813217639923096,
      "learning_rate": 1.1187985150185624e-05,
      "loss": 0.4992,
      "step": 11200
    },
    {
      "epoch": 1.892711099869314,
      "grad_norm": 0.7871003150939941,
      "learning_rate": 1.0766115423557207e-05,
      "loss": 0.4246,
      "step": 11225
    },
    {
      "epoch": 1.896926773744783,
      "grad_norm": 0.6627330183982849,
      "learning_rate": 1.034424569692879e-05,
      "loss": 0.4554,
      "step": 11250
    },
    {
      "epoch": 1.901142447620252,
      "grad_norm": 0.24951380491256714,
      "learning_rate": 9.922375970300373e-06,
      "loss": 0.5017,
      "step": 11275
    },
    {
      "epoch": 1.9053581214957211,
      "grad_norm": 0.7690525054931641,
      "learning_rate": 9.500506243671954e-06,
      "loss": 0.4841,
      "step": 11300
    },
    {
      "epoch": 1.9095737953711902,
      "grad_norm": 0.38569456338882446,
      "learning_rate": 9.078636517043537e-06,
      "loss": 0.5099,
      "step": 11325
    },
    {
      "epoch": 1.913789469246659,
      "grad_norm": 0.6501491665840149,
      "learning_rate": 8.65676679041512e-06,
      "loss": 0.55,
      "step": 11350
    },
    {
      "epoch": 1.918005143122128,
      "grad_norm": 0.49864381551742554,
      "learning_rate": 8.234897063786704e-06,
      "loss": 0.4585,
      "step": 11375
    },
    {
      "epoch": 1.922220816997597,
      "grad_norm": 0.44838589429855347,
      "learning_rate": 7.813027337158287e-06,
      "loss": 0.4191,
      "step": 11400
    },
    {
      "epoch": 1.926436490873066,
      "grad_norm": 1.1836965084075928,
      "learning_rate": 7.391157610529868e-06,
      "loss": 0.4252,
      "step": 11425
    },
    {
      "epoch": 1.930652164748535,
      "grad_norm": 0.8824089169502258,
      "learning_rate": 6.969287883901451e-06,
      "loss": 0.469,
      "step": 11450
    },
    {
      "epoch": 1.9348678386240041,
      "grad_norm": 0.7578915357589722,
      "learning_rate": 6.547418157273034e-06,
      "loss": 0.4433,
      "step": 11475
    },
    {
      "epoch": 1.9390835124994732,
      "grad_norm": 0.9477299451828003,
      "learning_rate": 6.125548430644617e-06,
      "loss": 0.4177,
      "step": 11500
    },
    {
      "epoch": 1.943299186374942,
      "grad_norm": 0.8000812530517578,
      "learning_rate": 5.7036787040162e-06,
      "loss": 0.4143,
      "step": 11525
    },
    {
      "epoch": 1.9475148602504109,
      "grad_norm": 0.7090982794761658,
      "learning_rate": 5.281808977387783e-06,
      "loss": 0.5135,
      "step": 11550
    },
    {
      "epoch": 1.95173053412588,
      "grad_norm": 0.6163873672485352,
      "learning_rate": 4.859939250759366e-06,
      "loss": 0.5488,
      "step": 11575
    },
    {
      "epoch": 1.955946208001349,
      "grad_norm": 0.22211810946464539,
      "learning_rate": 4.438069524130949e-06,
      "loss": 0.4613,
      "step": 11600
    },
    {
      "epoch": 1.960161881876818,
      "grad_norm": 0.6322907209396362,
      "learning_rate": 4.016199797502532e-06,
      "loss": 0.4743,
      "step": 11625
    },
    {
      "epoch": 1.9643775557522871,
      "grad_norm": 0.7233957052230835,
      "learning_rate": 3.594330070874114e-06,
      "loss": 0.4679,
      "step": 11650
    },
    {
      "epoch": 1.968593229627756,
      "grad_norm": 0.5567851066589355,
      "learning_rate": 3.1724603442456974e-06,
      "loss": 0.4794,
      "step": 11675
    },
    {
      "epoch": 1.972808903503225,
      "grad_norm": 0.8678857088088989,
      "learning_rate": 2.75059061761728e-06,
      "loss": 0.4619,
      "step": 11700
    },
    {
      "epoch": 1.9770245773786939,
      "grad_norm": 0.8775947690010071,
      "learning_rate": 2.3287208909888627e-06,
      "loss": 0.5452,
      "step": 11725
    },
    {
      "epoch": 1.981240251254163,
      "grad_norm": 0.845472514629364,
      "learning_rate": 1.9068511643604455e-06,
      "loss": 0.4865,
      "step": 11750
    },
    {
      "epoch": 1.985455925129632,
      "grad_norm": 0.24333499372005463,
      "learning_rate": 1.4849814377320284e-06,
      "loss": 0.4324,
      "step": 11775
    },
    {
      "epoch": 1.989671599005101,
      "grad_norm": 0.6740685105323792,
      "learning_rate": 1.0631117111036112e-06,
      "loss": 0.4566,
      "step": 11800
    },
    {
      "epoch": 1.99388727288057,
      "grad_norm": 0.7895365357398987,
      "learning_rate": 6.412419844751941e-07,
      "loss": 0.4608,
      "step": 11825
    },
    {
      "epoch": 1.998102946756039,
      "grad_norm": 0.9175684452056885,
      "learning_rate": 2.1937225784677695e-07,
      "loss": 0.5031,
      "step": 11850
    }
  ],
  "logging_steps": 25,
  "max_steps": 11862,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 5931,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.6298971431341466e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
