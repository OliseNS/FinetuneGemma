{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 5931,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004215673875468994,
      "grad_norm": 0.6592884063720703,
      "learning_rate": 0.00019976375295308808,
      "loss": 1.5586,
      "step": 25
    },
    {
      "epoch": 0.008431347750937988,
      "grad_norm": 0.6036758422851562,
      "learning_rate": 0.00019934188322645968,
      "loss": 0.7879,
      "step": 50
    },
    {
      "epoch": 0.012647021626406981,
      "grad_norm": 0.5132383108139038,
      "learning_rate": 0.00019892001349983127,
      "loss": 0.702,
      "step": 75
    },
    {
      "epoch": 0.016862695501875975,
      "grad_norm": 0.39161258935928345,
      "learning_rate": 0.00019849814377320284,
      "loss": 0.7755,
      "step": 100
    },
    {
      "epoch": 0.021078369377344967,
      "grad_norm": 0.42830291390419006,
      "learning_rate": 0.00019807627404657443,
      "loss": 0.6384,
      "step": 125
    },
    {
      "epoch": 0.025294043252813963,
      "grad_norm": 0.5469884872436523,
      "learning_rate": 0.00019765440431994603,
      "loss": 0.7503,
      "step": 150
    },
    {
      "epoch": 0.029509717128282955,
      "grad_norm": 0.42612940073013306,
      "learning_rate": 0.0001972325345933176,
      "loss": 0.7441,
      "step": 175
    },
    {
      "epoch": 0.03372539100375195,
      "grad_norm": 0.5841566920280457,
      "learning_rate": 0.00019681066486668916,
      "loss": 0.729,
      "step": 200
    },
    {
      "epoch": 0.03794106487922094,
      "grad_norm": 0.5062210559844971,
      "learning_rate": 0.00019638879514006076,
      "loss": 0.6978,
      "step": 225
    },
    {
      "epoch": 0.042156738754689935,
      "grad_norm": 0.3573777973651886,
      "learning_rate": 0.00019596692541343233,
      "loss": 0.7246,
      "step": 250
    },
    {
      "epoch": 0.046372412630158934,
      "grad_norm": 0.6388674974441528,
      "learning_rate": 0.00019554505568680392,
      "loss": 0.6521,
      "step": 275
    },
    {
      "epoch": 0.050588086505627926,
      "grad_norm": 0.3178512752056122,
      "learning_rate": 0.00019512318596017552,
      "loss": 0.7631,
      "step": 300
    },
    {
      "epoch": 0.05480376038109692,
      "grad_norm": 0.3294990658760071,
      "learning_rate": 0.00019470131623354708,
      "loss": 0.6467,
      "step": 325
    },
    {
      "epoch": 0.05901943425656591,
      "grad_norm": 0.458615243434906,
      "learning_rate": 0.00019427944650691868,
      "loss": 0.5709,
      "step": 350
    },
    {
      "epoch": 0.06323510813203491,
      "grad_norm": 0.5414772629737854,
      "learning_rate": 0.00019385757678029027,
      "loss": 0.7259,
      "step": 375
    },
    {
      "epoch": 0.0674507820075039,
      "grad_norm": 0.4126910865306854,
      "learning_rate": 0.00019343570705366184,
      "loss": 0.7192,
      "step": 400
    },
    {
      "epoch": 0.0716664558829729,
      "grad_norm": 0.287296861410141,
      "learning_rate": 0.0001930138373270334,
      "loss": 0.7791,
      "step": 425
    },
    {
      "epoch": 0.07588212975844189,
      "grad_norm": 0.5636021494865417,
      "learning_rate": 0.000192591967600405,
      "loss": 0.6891,
      "step": 450
    },
    {
      "epoch": 0.08009780363391088,
      "grad_norm": 0.5874422788619995,
      "learning_rate": 0.00019217009787377657,
      "loss": 0.679,
      "step": 475
    },
    {
      "epoch": 0.08431347750937987,
      "grad_norm": 0.4948570430278778,
      "learning_rate": 0.00019174822814714817,
      "loss": 0.7654,
      "step": 500
    },
    {
      "epoch": 0.08852915138484886,
      "grad_norm": 0.471447229385376,
      "learning_rate": 0.00019132635842051976,
      "loss": 0.6485,
      "step": 525
    },
    {
      "epoch": 0.09274482526031787,
      "grad_norm": 0.5673883557319641,
      "learning_rate": 0.00019090448869389133,
      "loss": 0.7328,
      "step": 550
    },
    {
      "epoch": 0.09696049913578686,
      "grad_norm": 0.48932039737701416,
      "learning_rate": 0.00019048261896726293,
      "loss": 0.6338,
      "step": 575
    },
    {
      "epoch": 0.10117617301125585,
      "grad_norm": 0.4578547775745392,
      "learning_rate": 0.0001900607492406345,
      "loss": 0.6501,
      "step": 600
    },
    {
      "epoch": 0.10539184688672484,
      "grad_norm": 0.38154685497283936,
      "learning_rate": 0.00018963887951400606,
      "loss": 0.6793,
      "step": 625
    },
    {
      "epoch": 0.10960752076219384,
      "grad_norm": 0.4200257956981659,
      "learning_rate": 0.00018921700978737766,
      "loss": 0.6818,
      "step": 650
    },
    {
      "epoch": 0.11382319463766283,
      "grad_norm": 0.3569240868091583,
      "learning_rate": 0.00018879514006074925,
      "loss": 0.6172,
      "step": 675
    },
    {
      "epoch": 0.11803886851313182,
      "grad_norm": 0.4966697096824646,
      "learning_rate": 0.00018837327033412082,
      "loss": 0.6623,
      "step": 700
    },
    {
      "epoch": 0.12225454238860081,
      "grad_norm": 0.3980119526386261,
      "learning_rate": 0.00018795140060749241,
      "loss": 0.6168,
      "step": 725
    },
    {
      "epoch": 0.12647021626406982,
      "grad_norm": 0.5363979339599609,
      "learning_rate": 0.000187529530880864,
      "loss": 0.7424,
      "step": 750
    },
    {
      "epoch": 0.1306858901395388,
      "grad_norm": 0.6548535823822021,
      "learning_rate": 0.0001871076611542356,
      "loss": 0.7289,
      "step": 775
    },
    {
      "epoch": 0.1349015640150078,
      "grad_norm": 0.46704715490341187,
      "learning_rate": 0.00018668579142760717,
      "loss": 0.6789,
      "step": 800
    },
    {
      "epoch": 0.13911723789047678,
      "grad_norm": 0.6546409726142883,
      "learning_rate": 0.00018626392170097874,
      "loss": 0.6294,
      "step": 825
    },
    {
      "epoch": 0.1433329117659458,
      "grad_norm": 0.5925196409225464,
      "learning_rate": 0.00018584205197435033,
      "loss": 0.5929,
      "step": 850
    },
    {
      "epoch": 0.1475485856414148,
      "grad_norm": 0.5094623565673828,
      "learning_rate": 0.0001854201822477219,
      "loss": 0.6491,
      "step": 875
    },
    {
      "epoch": 0.15176425951688377,
      "grad_norm": 0.5576695203781128,
      "learning_rate": 0.0001849983125210935,
      "loss": 0.6633,
      "step": 900
    },
    {
      "epoch": 0.15597993339235278,
      "grad_norm": 0.656904935836792,
      "learning_rate": 0.0001845764427944651,
      "loss": 0.5578,
      "step": 925
    },
    {
      "epoch": 0.16019560726782175,
      "grad_norm": 0.5915118455886841,
      "learning_rate": 0.00018415457306783666,
      "loss": 0.6476,
      "step": 950
    },
    {
      "epoch": 0.16441128114329076,
      "grad_norm": 0.47110211849212646,
      "learning_rate": 0.00018373270334120825,
      "loss": 0.6094,
      "step": 975
    },
    {
      "epoch": 0.16862695501875974,
      "grad_norm": 0.5799053907394409,
      "learning_rate": 0.00018331083361457982,
      "loss": 0.6819,
      "step": 1000
    },
    {
      "epoch": 0.17284262889422874,
      "grad_norm": 0.5956956744194031,
      "learning_rate": 0.0001828889638879514,
      "loss": 0.6597,
      "step": 1025
    },
    {
      "epoch": 0.17705830276969772,
      "grad_norm": 0.5662257075309753,
      "learning_rate": 0.00018246709416132298,
      "loss": 0.6691,
      "step": 1050
    },
    {
      "epoch": 0.18127397664516673,
      "grad_norm": 0.4691295921802521,
      "learning_rate": 0.00018204522443469458,
      "loss": 0.6728,
      "step": 1075
    },
    {
      "epoch": 0.18548965052063573,
      "grad_norm": 0.4611334800720215,
      "learning_rate": 0.00018162335470806615,
      "loss": 0.6886,
      "step": 1100
    },
    {
      "epoch": 0.1897053243961047,
      "grad_norm": 0.5494567155838013,
      "learning_rate": 0.00018120148498143774,
      "loss": 0.6516,
      "step": 1125
    },
    {
      "epoch": 0.19392099827157372,
      "grad_norm": 0.5572909116744995,
      "learning_rate": 0.00018077961525480934,
      "loss": 0.7555,
      "step": 1150
    },
    {
      "epoch": 0.1981366721470427,
      "grad_norm": 0.615109920501709,
      "learning_rate": 0.0001803577455281809,
      "loss": 0.5759,
      "step": 1175
    },
    {
      "epoch": 0.2023523460225117,
      "grad_norm": 0.5771766304969788,
      "learning_rate": 0.0001799358758015525,
      "loss": 0.7107,
      "step": 1200
    },
    {
      "epoch": 0.20656801989798068,
      "grad_norm": 0.5355958938598633,
      "learning_rate": 0.00017951400607492407,
      "loss": 0.6695,
      "step": 1225
    },
    {
      "epoch": 0.2107836937734497,
      "grad_norm": 0.5520287156105042,
      "learning_rate": 0.00017909213634829564,
      "loss": 0.6799,
      "step": 1250
    },
    {
      "epoch": 0.2149993676489187,
      "grad_norm": 0.5686078667640686,
      "learning_rate": 0.00017867026662166723,
      "loss": 0.6833,
      "step": 1275
    },
    {
      "epoch": 0.21921504152438767,
      "grad_norm": 0.5358233451843262,
      "learning_rate": 0.00017824839689503883,
      "loss": 0.7612,
      "step": 1300
    },
    {
      "epoch": 0.22343071539985668,
      "grad_norm": 0.44661596417427063,
      "learning_rate": 0.0001778265271684104,
      "loss": 0.6023,
      "step": 1325
    },
    {
      "epoch": 0.22764638927532566,
      "grad_norm": 0.5983580350875854,
      "learning_rate": 0.000177404657441782,
      "loss": 0.6756,
      "step": 1350
    },
    {
      "epoch": 0.23186206315079466,
      "grad_norm": 0.49695885181427,
      "learning_rate": 0.00017698278771515358,
      "loss": 0.6665,
      "step": 1375
    },
    {
      "epoch": 0.23607773702626364,
      "grad_norm": 0.5483479499816895,
      "learning_rate": 0.00017656091798852515,
      "loss": 0.66,
      "step": 1400
    },
    {
      "epoch": 0.24029341090173265,
      "grad_norm": 0.6481131911277771,
      "learning_rate": 0.00017613904826189675,
      "loss": 0.5954,
      "step": 1425
    },
    {
      "epoch": 0.24450908477720162,
      "grad_norm": 0.5621160268783569,
      "learning_rate": 0.00017571717853526831,
      "loss": 0.707,
      "step": 1450
    },
    {
      "epoch": 0.24872475865267063,
      "grad_norm": 0.5550326108932495,
      "learning_rate": 0.00017529530880863988,
      "loss": 0.6561,
      "step": 1475
    },
    {
      "epoch": 0.25294043252813964,
      "grad_norm": 0.4128023684024811,
      "learning_rate": 0.00017487343908201148,
      "loss": 0.6088,
      "step": 1500
    },
    {
      "epoch": 0.25715610640360864,
      "grad_norm": 0.37165990471839905,
      "learning_rate": 0.00017445156935538307,
      "loss": 0.5795,
      "step": 1525
    },
    {
      "epoch": 0.2613717802790776,
      "grad_norm": 0.3601559102535248,
      "learning_rate": 0.00017402969962875464,
      "loss": 0.6203,
      "step": 1550
    },
    {
      "epoch": 0.2655874541545466,
      "grad_norm": 0.578576922416687,
      "learning_rate": 0.00017360782990212623,
      "loss": 0.597,
      "step": 1575
    },
    {
      "epoch": 0.2698031280300156,
      "grad_norm": 0.590861976146698,
      "learning_rate": 0.00017318596017549783,
      "loss": 0.689,
      "step": 1600
    },
    {
      "epoch": 0.2740188019054846,
      "grad_norm": 0.5369037389755249,
      "learning_rate": 0.0001727640904488694,
      "loss": 0.6618,
      "step": 1625
    },
    {
      "epoch": 0.27823447578095356,
      "grad_norm": 0.5017134547233582,
      "learning_rate": 0.00017234222072224096,
      "loss": 0.6494,
      "step": 1650
    },
    {
      "epoch": 0.28245014965642257,
      "grad_norm": 0.6125921010971069,
      "learning_rate": 0.00017192035099561256,
      "loss": 0.6709,
      "step": 1675
    },
    {
      "epoch": 0.2866658235318916,
      "grad_norm": 0.484631210565567,
      "learning_rate": 0.00017149848126898415,
      "loss": 0.6782,
      "step": 1700
    },
    {
      "epoch": 0.2908814974073606,
      "grad_norm": 0.6422034502029419,
      "learning_rate": 0.00017107661154235572,
      "loss": 0.7497,
      "step": 1725
    },
    {
      "epoch": 0.2950971712828296,
      "grad_norm": 0.577578604221344,
      "learning_rate": 0.00017065474181572732,
      "loss": 0.6431,
      "step": 1750
    },
    {
      "epoch": 0.29931284515829853,
      "grad_norm": 0.6019136905670166,
      "learning_rate": 0.0001702328720890989,
      "loss": 0.6948,
      "step": 1775
    },
    {
      "epoch": 0.30352851903376754,
      "grad_norm": 0.6077538728713989,
      "learning_rate": 0.00016981100236247048,
      "loss": 0.6753,
      "step": 1800
    },
    {
      "epoch": 0.30774419290923655,
      "grad_norm": 0.42457517981529236,
      "learning_rate": 0.00016938913263584208,
      "loss": 0.5985,
      "step": 1825
    },
    {
      "epoch": 0.31195986678470555,
      "grad_norm": 0.5110180974006653,
      "learning_rate": 0.00016896726290921364,
      "loss": 0.5922,
      "step": 1850
    },
    {
      "epoch": 0.3161755406601745,
      "grad_norm": 0.6813841462135315,
      "learning_rate": 0.0001685453931825852,
      "loss": 0.6376,
      "step": 1875
    },
    {
      "epoch": 0.3203912145356435,
      "grad_norm": 0.41532841324806213,
      "learning_rate": 0.0001681235234559568,
      "loss": 0.6212,
      "step": 1900
    },
    {
      "epoch": 0.3246068884111125,
      "grad_norm": 0.4543830454349518,
      "learning_rate": 0.0001677016537293284,
      "loss": 0.5767,
      "step": 1925
    },
    {
      "epoch": 0.3288225622865815,
      "grad_norm": 0.38953709602355957,
      "learning_rate": 0.00016727978400269997,
      "loss": 0.7119,
      "step": 1950
    },
    {
      "epoch": 0.3330382361620505,
      "grad_norm": 0.6097630262374878,
      "learning_rate": 0.00016685791427607156,
      "loss": 0.6543,
      "step": 1975
    },
    {
      "epoch": 0.3372539100375195,
      "grad_norm": 0.42704203724861145,
      "learning_rate": 0.00016643604454944316,
      "loss": 0.6843,
      "step": 2000
    },
    {
      "epoch": 0.3414695839129885,
      "grad_norm": 0.4541128873825073,
      "learning_rate": 0.00016601417482281473,
      "loss": 0.6258,
      "step": 2025
    },
    {
      "epoch": 0.3456852577884575,
      "grad_norm": 0.6625218987464905,
      "learning_rate": 0.0001655923050961863,
      "loss": 0.6967,
      "step": 2050
    },
    {
      "epoch": 0.3499009316639265,
      "grad_norm": 0.6480218172073364,
      "learning_rate": 0.0001651704353695579,
      "loss": 0.5992,
      "step": 2075
    },
    {
      "epoch": 0.35411660553939545,
      "grad_norm": 0.5420606732368469,
      "learning_rate": 0.00016474856564292946,
      "loss": 0.6865,
      "step": 2100
    },
    {
      "epoch": 0.35833227941486445,
      "grad_norm": 0.5245200991630554,
      "learning_rate": 0.00016432669591630105,
      "loss": 0.6452,
      "step": 2125
    },
    {
      "epoch": 0.36254795329033346,
      "grad_norm": 0.40461981296539307,
      "learning_rate": 0.00016390482618967265,
      "loss": 0.6156,
      "step": 2150
    },
    {
      "epoch": 0.36676362716580246,
      "grad_norm": 0.48923876881599426,
      "learning_rate": 0.00016348295646304421,
      "loss": 0.589,
      "step": 2175
    },
    {
      "epoch": 0.37097930104127147,
      "grad_norm": 0.5824174284934998,
      "learning_rate": 0.0001630610867364158,
      "loss": 0.5449,
      "step": 2200
    },
    {
      "epoch": 0.3751949749167404,
      "grad_norm": 0.7029809355735779,
      "learning_rate": 0.0001626392170097874,
      "loss": 0.6917,
      "step": 2225
    },
    {
      "epoch": 0.3794106487922094,
      "grad_norm": 0.5921729803085327,
      "learning_rate": 0.00016221734728315897,
      "loss": 0.6561,
      "step": 2250
    },
    {
      "epoch": 0.38362632266767843,
      "grad_norm": 0.5674454569816589,
      "learning_rate": 0.00016179547755653054,
      "loss": 0.6677,
      "step": 2275
    },
    {
      "epoch": 0.38784199654314744,
      "grad_norm": 0.610201895236969,
      "learning_rate": 0.00016137360782990213,
      "loss": 0.5913,
      "step": 2300
    },
    {
      "epoch": 0.3920576704186164,
      "grad_norm": 0.5491762757301331,
      "learning_rate": 0.0001609517381032737,
      "loss": 0.5927,
      "step": 2325
    },
    {
      "epoch": 0.3962733442940854,
      "grad_norm": 0.3530021011829376,
      "learning_rate": 0.0001605298683766453,
      "loss": 0.6655,
      "step": 2350
    },
    {
      "epoch": 0.4004890181695544,
      "grad_norm": 0.7165349125862122,
      "learning_rate": 0.0001601079986500169,
      "loss": 0.6757,
      "step": 2375
    },
    {
      "epoch": 0.4047046920450234,
      "grad_norm": 0.6272157430648804,
      "learning_rate": 0.00015968612892338846,
      "loss": 0.6476,
      "step": 2400
    },
    {
      "epoch": 0.4089203659204924,
      "grad_norm": 0.47820591926574707,
      "learning_rate": 0.00015926425919676006,
      "loss": 0.6039,
      "step": 2425
    },
    {
      "epoch": 0.41313603979596136,
      "grad_norm": 0.6272923350334167,
      "learning_rate": 0.00015884238947013162,
      "loss": 0.5705,
      "step": 2450
    },
    {
      "epoch": 0.41735171367143037,
      "grad_norm": 0.6876711845397949,
      "learning_rate": 0.0001584205197435032,
      "loss": 0.6635,
      "step": 2475
    },
    {
      "epoch": 0.4215673875468994,
      "grad_norm": 0.5813876390457153,
      "learning_rate": 0.00015799865001687479,
      "loss": 0.6998,
      "step": 2500
    },
    {
      "epoch": 0.4257830614223684,
      "grad_norm": 0.5410722494125366,
      "learning_rate": 0.00015757678029024638,
      "loss": 0.6498,
      "step": 2525
    },
    {
      "epoch": 0.4299987352978374,
      "grad_norm": 0.4489375650882721,
      "learning_rate": 0.00015715491056361795,
      "loss": 0.6523,
      "step": 2550
    },
    {
      "epoch": 0.43421440917330634,
      "grad_norm": 0.32918059825897217,
      "learning_rate": 0.00015673304083698954,
      "loss": 0.5475,
      "step": 2575
    },
    {
      "epoch": 0.43843008304877534,
      "grad_norm": 0.6290420889854431,
      "learning_rate": 0.00015631117111036114,
      "loss": 0.5817,
      "step": 2600
    },
    {
      "epoch": 0.44264575692424435,
      "grad_norm": 0.5697997212409973,
      "learning_rate": 0.0001558893013837327,
      "loss": 0.6317,
      "step": 2625
    },
    {
      "epoch": 0.44686143079971336,
      "grad_norm": 0.47042548656463623,
      "learning_rate": 0.0001554674316571043,
      "loss": 0.6212,
      "step": 2650
    },
    {
      "epoch": 0.4510771046751823,
      "grad_norm": 0.4709092676639557,
      "learning_rate": 0.00015504556193047587,
      "loss": 0.6935,
      "step": 2675
    },
    {
      "epoch": 0.4552927785506513,
      "grad_norm": 0.43760597705841064,
      "learning_rate": 0.00015462369220384746,
      "loss": 0.5555,
      "step": 2700
    },
    {
      "epoch": 0.4595084524261203,
      "grad_norm": 0.6382158398628235,
      "learning_rate": 0.00015420182247721903,
      "loss": 0.665,
      "step": 2725
    },
    {
      "epoch": 0.4637241263015893,
      "grad_norm": 0.5693995952606201,
      "learning_rate": 0.00015377995275059063,
      "loss": 0.6546,
      "step": 2750
    },
    {
      "epoch": 0.46793980017705833,
      "grad_norm": 0.6461164355278015,
      "learning_rate": 0.00015335808302396222,
      "loss": 0.5976,
      "step": 2775
    },
    {
      "epoch": 0.4721554740525273,
      "grad_norm": 0.5676155090332031,
      "learning_rate": 0.0001529362132973338,
      "loss": 0.6376,
      "step": 2800
    },
    {
      "epoch": 0.4763711479279963,
      "grad_norm": 0.3362257480621338,
      "learning_rate": 0.00015251434357070538,
      "loss": 0.46,
      "step": 2825
    },
    {
      "epoch": 0.4805868218034653,
      "grad_norm": 0.4114580750465393,
      "learning_rate": 0.00015209247384407698,
      "loss": 0.6503,
      "step": 2850
    },
    {
      "epoch": 0.4848024956789343,
      "grad_norm": 0.37636545300483704,
      "learning_rate": 0.00015167060411744855,
      "loss": 0.6536,
      "step": 2875
    },
    {
      "epoch": 0.48901816955440325,
      "grad_norm": 0.7761818170547485,
      "learning_rate": 0.00015124873439082011,
      "loss": 0.6606,
      "step": 2900
    },
    {
      "epoch": 0.49323384342987225,
      "grad_norm": 0.541912853717804,
      "learning_rate": 0.0001508268646641917,
      "loss": 0.6287,
      "step": 2925
    },
    {
      "epoch": 0.49744951730534126,
      "grad_norm": 0.5690341591835022,
      "learning_rate": 0.00015040499493756328,
      "loss": 0.6333,
      "step": 2950
    },
    {
      "epoch": 0.5016651911808102,
      "grad_norm": 0.65323805809021,
      "learning_rate": 0.00014998312521093487,
      "loss": 0.6275,
      "step": 2975
    },
    {
      "epoch": 0.5058808650562793,
      "grad_norm": 0.6973729133605957,
      "learning_rate": 0.00014956125548430647,
      "loss": 0.6629,
      "step": 3000
    },
    {
      "epoch": 0.5100965389317482,
      "grad_norm": 0.5254755616188049,
      "learning_rate": 0.00014913938575767804,
      "loss": 0.7219,
      "step": 3025
    },
    {
      "epoch": 0.5143122128072173,
      "grad_norm": 0.6172437071800232,
      "learning_rate": 0.00014871751603104963,
      "loss": 0.6933,
      "step": 3050
    },
    {
      "epoch": 0.5185278866826862,
      "grad_norm": 0.3288564085960388,
      "learning_rate": 0.0001482956463044212,
      "loss": 0.573,
      "step": 3075
    },
    {
      "epoch": 0.5227435605581552,
      "grad_norm": 0.6276848316192627,
      "learning_rate": 0.00014787377657779277,
      "loss": 0.6586,
      "step": 3100
    },
    {
      "epoch": 0.5269592344336242,
      "grad_norm": 0.4938656687736511,
      "learning_rate": 0.00014745190685116436,
      "loss": 0.7353,
      "step": 3125
    },
    {
      "epoch": 0.5311749083090932,
      "grad_norm": 0.7912372946739197,
      "learning_rate": 0.00014703003712453596,
      "loss": 0.6351,
      "step": 3150
    },
    {
      "epoch": 0.5353905821845621,
      "grad_norm": 0.34613236784935,
      "learning_rate": 0.00014660816739790752,
      "loss": 0.7467,
      "step": 3175
    },
    {
      "epoch": 0.5396062560600312,
      "grad_norm": 0.6180621981620789,
      "learning_rate": 0.00014618629767127912,
      "loss": 0.5866,
      "step": 3200
    },
    {
      "epoch": 0.5438219299355002,
      "grad_norm": 0.3914062976837158,
      "learning_rate": 0.0001457644279446507,
      "loss": 0.5676,
      "step": 3225
    },
    {
      "epoch": 0.5480376038109692,
      "grad_norm": 0.5602525472640991,
      "learning_rate": 0.00014534255821802228,
      "loss": 0.5428,
      "step": 3250
    },
    {
      "epoch": 0.5522532776864382,
      "grad_norm": 0.5735276341438293,
      "learning_rate": 0.00014492068849139388,
      "loss": 0.6456,
      "step": 3275
    },
    {
      "epoch": 0.5564689515619071,
      "grad_norm": 0.34951359033584595,
      "learning_rate": 0.00014449881876476544,
      "loss": 0.5155,
      "step": 3300
    },
    {
      "epoch": 0.5606846254373762,
      "grad_norm": 0.6517199873924255,
      "learning_rate": 0.000144076949038137,
      "loss": 0.5248,
      "step": 3325
    },
    {
      "epoch": 0.5649002993128451,
      "grad_norm": 0.4959523677825928,
      "learning_rate": 0.0001436550793115086,
      "loss": 0.5785,
      "step": 3350
    },
    {
      "epoch": 0.5691159731883142,
      "grad_norm": 0.6868893504142761,
      "learning_rate": 0.0001432332095848802,
      "loss": 0.6247,
      "step": 3375
    },
    {
      "epoch": 0.5733316470637831,
      "grad_norm": 0.6509615182876587,
      "learning_rate": 0.00014281133985825177,
      "loss": 0.5059,
      "step": 3400
    },
    {
      "epoch": 0.5775473209392521,
      "grad_norm": 0.508507490158081,
      "learning_rate": 0.00014238947013162336,
      "loss": 0.6269,
      "step": 3425
    },
    {
      "epoch": 0.5817629948147212,
      "grad_norm": 0.4920637905597687,
      "learning_rate": 0.00014196760040499496,
      "loss": 0.6251,
      "step": 3450
    },
    {
      "epoch": 0.5859786686901901,
      "grad_norm": 0.2905634343624115,
      "learning_rate": 0.00014154573067836653,
      "loss": 0.5203,
      "step": 3475
    },
    {
      "epoch": 0.5901943425656592,
      "grad_norm": 0.6269543170928955,
      "learning_rate": 0.0001411238609517381,
      "loss": 0.6456,
      "step": 3500
    },
    {
      "epoch": 0.5944100164411281,
      "grad_norm": 0.5851781368255615,
      "learning_rate": 0.0001407019912251097,
      "loss": 0.5949,
      "step": 3525
    },
    {
      "epoch": 0.5986256903165971,
      "grad_norm": 0.5535221695899963,
      "learning_rate": 0.00014028012149848126,
      "loss": 0.5812,
      "step": 3550
    },
    {
      "epoch": 0.6028413641920661,
      "grad_norm": 0.6514847278594971,
      "learning_rate": 0.00013985825177185285,
      "loss": 0.5112,
      "step": 3575
    },
    {
      "epoch": 0.6070570380675351,
      "grad_norm": 0.6617153286933899,
      "learning_rate": 0.00013943638204522445,
      "loss": 0.5754,
      "step": 3600
    },
    {
      "epoch": 0.6112727119430041,
      "grad_norm": 0.5161499977111816,
      "learning_rate": 0.00013901451231859604,
      "loss": 0.5725,
      "step": 3625
    },
    {
      "epoch": 0.6154883858184731,
      "grad_norm": 0.6644999980926514,
      "learning_rate": 0.0001385926425919676,
      "loss": 0.6319,
      "step": 3650
    },
    {
      "epoch": 0.619704059693942,
      "grad_norm": 0.5403600931167603,
      "learning_rate": 0.0001381707728653392,
      "loss": 0.5136,
      "step": 3675
    },
    {
      "epoch": 0.6239197335694111,
      "grad_norm": 0.5741940140724182,
      "learning_rate": 0.00013774890313871077,
      "loss": 0.5649,
      "step": 3700
    },
    {
      "epoch": 0.6281354074448801,
      "grad_norm": 0.5928864479064941,
      "learning_rate": 0.00013732703341208234,
      "loss": 0.593,
      "step": 3725
    },
    {
      "epoch": 0.632351081320349,
      "grad_norm": 0.6705028414726257,
      "learning_rate": 0.00013690516368545394,
      "loss": 0.5893,
      "step": 3750
    },
    {
      "epoch": 0.6365667551958181,
      "grad_norm": 0.6181473731994629,
      "learning_rate": 0.00013648329395882553,
      "loss": 0.6296,
      "step": 3775
    },
    {
      "epoch": 0.640782429071287,
      "grad_norm": 0.2618084251880646,
      "learning_rate": 0.0001360614242321971,
      "loss": 0.6064,
      "step": 3800
    },
    {
      "epoch": 0.6449981029467561,
      "grad_norm": 0.7133391499519348,
      "learning_rate": 0.0001356395545055687,
      "loss": 0.5727,
      "step": 3825
    },
    {
      "epoch": 0.649213776822225,
      "grad_norm": 0.2838265001773834,
      "learning_rate": 0.0001352176847789403,
      "loss": 0.5874,
      "step": 3850
    },
    {
      "epoch": 0.653429450697694,
      "grad_norm": 0.6463882327079773,
      "learning_rate": 0.00013479581505231186,
      "loss": 0.6441,
      "step": 3875
    },
    {
      "epoch": 0.657645124573163,
      "grad_norm": 0.5738253593444824,
      "learning_rate": 0.00013437394532568342,
      "loss": 0.6203,
      "step": 3900
    },
    {
      "epoch": 0.661860798448632,
      "grad_norm": 0.6987646222114563,
      "learning_rate": 0.00013395207559905502,
      "loss": 0.6381,
      "step": 3925
    },
    {
      "epoch": 0.666076472324101,
      "grad_norm": 0.5635076761245728,
      "learning_rate": 0.00013353020587242659,
      "loss": 0.6958,
      "step": 3950
    },
    {
      "epoch": 0.67029214619957,
      "grad_norm": 0.4619535207748413,
      "learning_rate": 0.00013310833614579818,
      "loss": 0.5727,
      "step": 3975
    },
    {
      "epoch": 0.674507820075039,
      "grad_norm": 0.4986385107040405,
      "learning_rate": 0.00013268646641916978,
      "loss": 0.6043,
      "step": 4000
    },
    {
      "epoch": 0.678723493950508,
      "grad_norm": 0.5768849849700928,
      "learning_rate": 0.00013226459669254134,
      "loss": 0.5865,
      "step": 4025
    },
    {
      "epoch": 0.682939167825977,
      "grad_norm": 0.6660186052322388,
      "learning_rate": 0.00013184272696591294,
      "loss": 0.5481,
      "step": 4050
    },
    {
      "epoch": 0.687154841701446,
      "grad_norm": 0.6398460865020752,
      "learning_rate": 0.00013142085723928453,
      "loss": 0.6831,
      "step": 4075
    },
    {
      "epoch": 0.691370515576915,
      "grad_norm": 0.5294440388679504,
      "learning_rate": 0.0001309989875126561,
      "loss": 0.5794,
      "step": 4100
    },
    {
      "epoch": 0.6955861894523839,
      "grad_norm": 0.5679708123207092,
      "learning_rate": 0.00013057711778602767,
      "loss": 0.5743,
      "step": 4125
    },
    {
      "epoch": 0.699801863327853,
      "grad_norm": 0.6143545508384705,
      "learning_rate": 0.00013015524805939926,
      "loss": 0.5992,
      "step": 4150
    },
    {
      "epoch": 0.7040175372033219,
      "grad_norm": 0.5241633057594299,
      "learning_rate": 0.00012973337833277083,
      "loss": 0.631,
      "step": 4175
    },
    {
      "epoch": 0.7082332110787909,
      "grad_norm": 0.5035251975059509,
      "learning_rate": 0.00012931150860614243,
      "loss": 0.506,
      "step": 4200
    },
    {
      "epoch": 0.71244888495426,
      "grad_norm": 0.38072308897972107,
      "learning_rate": 0.00012888963887951402,
      "loss": 0.6068,
      "step": 4225
    },
    {
      "epoch": 0.7166645588297289,
      "grad_norm": 0.5499347448348999,
      "learning_rate": 0.0001284677691528856,
      "loss": 0.5837,
      "step": 4250
    },
    {
      "epoch": 0.720880232705198,
      "grad_norm": 0.48047101497650146,
      "learning_rate": 0.00012804589942625718,
      "loss": 0.5545,
      "step": 4275
    },
    {
      "epoch": 0.7250959065806669,
      "grad_norm": 0.4561833441257477,
      "learning_rate": 0.00012762402969962878,
      "loss": 0.6017,
      "step": 4300
    },
    {
      "epoch": 0.7293115804561359,
      "grad_norm": 0.467801958322525,
      "learning_rate": 0.00012720215997300035,
      "loss": 0.5628,
      "step": 4325
    },
    {
      "epoch": 0.7335272543316049,
      "grad_norm": 0.5959245562553406,
      "learning_rate": 0.00012678029024637192,
      "loss": 0.6,
      "step": 4350
    },
    {
      "epoch": 0.7377429282070739,
      "grad_norm": 0.7162978649139404,
      "learning_rate": 0.0001263584205197435,
      "loss": 0.6256,
      "step": 4375
    },
    {
      "epoch": 0.7419586020825429,
      "grad_norm": 0.3827458918094635,
      "learning_rate": 0.00012593655079311508,
      "loss": 0.5855,
      "step": 4400
    },
    {
      "epoch": 0.7461742759580119,
      "grad_norm": 0.6210541129112244,
      "learning_rate": 0.00012551468106648667,
      "loss": 0.7042,
      "step": 4425
    },
    {
      "epoch": 0.7503899498334808,
      "grad_norm": 0.7009960412979126,
      "learning_rate": 0.00012509281133985827,
      "loss": 0.675,
      "step": 4450
    },
    {
      "epoch": 0.7546056237089499,
      "grad_norm": 0.4742926359176636,
      "learning_rate": 0.00012467094161322984,
      "loss": 0.5351,
      "step": 4475
    },
    {
      "epoch": 0.7588212975844189,
      "grad_norm": 0.6871809959411621,
      "learning_rate": 0.00012424907188660143,
      "loss": 0.6094,
      "step": 4500
    },
    {
      "epoch": 0.7630369714598879,
      "grad_norm": 0.6019153594970703,
      "learning_rate": 0.000123827202159973,
      "loss": 0.5696,
      "step": 4525
    },
    {
      "epoch": 0.7672526453353569,
      "grad_norm": 0.5854367017745972,
      "learning_rate": 0.00012340533243334457,
      "loss": 0.596,
      "step": 4550
    },
    {
      "epoch": 0.7714683192108258,
      "grad_norm": 0.6882818937301636,
      "learning_rate": 0.00012298346270671616,
      "loss": 0.5454,
      "step": 4575
    },
    {
      "epoch": 0.7756839930862949,
      "grad_norm": 0.4133595824241638,
      "learning_rate": 0.00012256159298008776,
      "loss": 0.549,
      "step": 4600
    },
    {
      "epoch": 0.7798996669617638,
      "grad_norm": 0.6454863548278809,
      "learning_rate": 0.00012213972325345935,
      "loss": 0.6063,
      "step": 4625
    },
    {
      "epoch": 0.7841153408372328,
      "grad_norm": 0.7342830896377563,
      "learning_rate": 0.00012171785352683092,
      "loss": 0.5275,
      "step": 4650
    },
    {
      "epoch": 0.7883310147127018,
      "grad_norm": 0.6497771143913269,
      "learning_rate": 0.0001212959838002025,
      "loss": 0.6337,
      "step": 4675
    },
    {
      "epoch": 0.7925466885881708,
      "grad_norm": 0.4732552170753479,
      "learning_rate": 0.0001208741140735741,
      "loss": 0.5323,
      "step": 4700
    },
    {
      "epoch": 0.7967623624636399,
      "grad_norm": 0.5097313523292542,
      "learning_rate": 0.00012045224434694566,
      "loss": 0.5899,
      "step": 4725
    },
    {
      "epoch": 0.8009780363391088,
      "grad_norm": 0.4787701368331909,
      "learning_rate": 0.00012003037462031726,
      "loss": 0.5,
      "step": 4750
    },
    {
      "epoch": 0.8051937102145778,
      "grad_norm": 0.5041419267654419,
      "learning_rate": 0.00011960850489368884,
      "loss": 0.628,
      "step": 4775
    },
    {
      "epoch": 0.8094093840900468,
      "grad_norm": 0.6417216658592224,
      "learning_rate": 0.00011918663516706041,
      "loss": 0.6521,
      "step": 4800
    },
    {
      "epoch": 0.8136250579655158,
      "grad_norm": 0.5072162747383118,
      "learning_rate": 0.000118764765440432,
      "loss": 0.5122,
      "step": 4825
    },
    {
      "epoch": 0.8178407318409848,
      "grad_norm": 0.29049667716026306,
      "learning_rate": 0.0001183428957138036,
      "loss": 0.5814,
      "step": 4850
    },
    {
      "epoch": 0.8220564057164538,
      "grad_norm": 0.7524094581604004,
      "learning_rate": 0.00011792102598717516,
      "loss": 0.607,
      "step": 4875
    },
    {
      "epoch": 0.8262720795919227,
      "grad_norm": 0.537179708480835,
      "learning_rate": 0.00011749915626054675,
      "loss": 0.5462,
      "step": 4900
    },
    {
      "epoch": 0.8304877534673918,
      "grad_norm": 0.4518299698829651,
      "learning_rate": 0.00011707728653391834,
      "loss": 0.5757,
      "step": 4925
    },
    {
      "epoch": 0.8347034273428607,
      "grad_norm": 0.4717993438243866,
      "learning_rate": 0.00011665541680728991,
      "loss": 0.6238,
      "step": 4950
    },
    {
      "epoch": 0.8389191012183298,
      "grad_norm": 0.7028119564056396,
      "learning_rate": 0.0001162335470806615,
      "loss": 0.5341,
      "step": 4975
    },
    {
      "epoch": 0.8431347750937987,
      "grad_norm": 0.47149431705474854,
      "learning_rate": 0.00011581167735403309,
      "loss": 0.568,
      "step": 5000
    },
    {
      "epoch": 0.8473504489692677,
      "grad_norm": 0.5332790613174438,
      "learning_rate": 0.00011538980762740465,
      "loss": 0.5213,
      "step": 5025
    },
    {
      "epoch": 0.8515661228447368,
      "grad_norm": 0.6513734459877014,
      "learning_rate": 0.00011496793790077625,
      "loss": 0.6385,
      "step": 5050
    },
    {
      "epoch": 0.8557817967202057,
      "grad_norm": 0.48159608244895935,
      "learning_rate": 0.00011454606817414783,
      "loss": 0.5239,
      "step": 5075
    },
    {
      "epoch": 0.8599974705956748,
      "grad_norm": 0.6819217205047607,
      "learning_rate": 0.0001141241984475194,
      "loss": 0.6119,
      "step": 5100
    },
    {
      "epoch": 0.8642131444711437,
      "grad_norm": 0.703839123249054,
      "learning_rate": 0.00011370232872089099,
      "loss": 0.5921,
      "step": 5125
    },
    {
      "epoch": 0.8684288183466127,
      "grad_norm": 0.5300304293632507,
      "learning_rate": 0.00011328045899426259,
      "loss": 0.4969,
      "step": 5150
    },
    {
      "epoch": 0.8726444922220817,
      "grad_norm": 0.24977460503578186,
      "learning_rate": 0.00011285858926763415,
      "loss": 0.4973,
      "step": 5175
    },
    {
      "epoch": 0.8768601660975507,
      "grad_norm": 0.436964213848114,
      "learning_rate": 0.00011243671954100574,
      "loss": 0.574,
      "step": 5200
    },
    {
      "epoch": 0.8810758399730196,
      "grad_norm": 0.6595659255981445,
      "learning_rate": 0.00011201484981437733,
      "loss": 0.5918,
      "step": 5225
    },
    {
      "epoch": 0.8852915138484887,
      "grad_norm": 0.7338206171989441,
      "learning_rate": 0.0001115929800877489,
      "loss": 0.6525,
      "step": 5250
    },
    {
      "epoch": 0.8895071877239576,
      "grad_norm": 0.5962433218955994,
      "learning_rate": 0.0001111711103611205,
      "loss": 0.5804,
      "step": 5275
    },
    {
      "epoch": 0.8937228615994267,
      "grad_norm": 0.6299694180488586,
      "learning_rate": 0.00011074924063449208,
      "loss": 0.5338,
      "step": 5300
    },
    {
      "epoch": 0.8979385354748957,
      "grad_norm": 0.6968527436256409,
      "learning_rate": 0.00011032737090786364,
      "loss": 0.6213,
      "step": 5325
    },
    {
      "epoch": 0.9021542093503646,
      "grad_norm": 0.35897061228752136,
      "learning_rate": 0.00010990550118123524,
      "loss": 0.6021,
      "step": 5350
    },
    {
      "epoch": 0.9063698832258337,
      "grad_norm": 0.6376249194145203,
      "learning_rate": 0.00010948363145460683,
      "loss": 0.5814,
      "step": 5375
    },
    {
      "epoch": 0.9105855571013026,
      "grad_norm": 0.7169381976127625,
      "learning_rate": 0.0001090617617279784,
      "loss": 0.5293,
      "step": 5400
    },
    {
      "epoch": 0.9148012309767717,
      "grad_norm": 0.6142295598983765,
      "learning_rate": 0.00010863989200134998,
      "loss": 0.5357,
      "step": 5425
    },
    {
      "epoch": 0.9190169048522406,
      "grad_norm": 0.29530566930770874,
      "learning_rate": 0.00010821802227472158,
      "loss": 0.5452,
      "step": 5450
    },
    {
      "epoch": 0.9232325787277096,
      "grad_norm": 0.6458328366279602,
      "learning_rate": 0.00010779615254809314,
      "loss": 0.6358,
      "step": 5475
    },
    {
      "epoch": 0.9274482526031786,
      "grad_norm": 0.5853328108787537,
      "learning_rate": 0.00010737428282146474,
      "loss": 0.5302,
      "step": 5500
    },
    {
      "epoch": 0.9316639264786476,
      "grad_norm": 0.5087891221046448,
      "learning_rate": 0.00010695241309483632,
      "loss": 0.6497,
      "step": 5525
    },
    {
      "epoch": 0.9358796003541167,
      "grad_norm": 0.5413863658905029,
      "learning_rate": 0.00010653054336820792,
      "loss": 0.6033,
      "step": 5550
    },
    {
      "epoch": 0.9400952742295856,
      "grad_norm": 0.5840063095092773,
      "learning_rate": 0.00010610867364157948,
      "loss": 0.549,
      "step": 5575
    },
    {
      "epoch": 0.9443109481050546,
      "grad_norm": 0.46717432141304016,
      "learning_rate": 0.00010568680391495107,
      "loss": 0.6153,
      "step": 5600
    },
    {
      "epoch": 0.9485266219805236,
      "grad_norm": 0.37896203994750977,
      "learning_rate": 0.00010526493418832266,
      "loss": 0.595,
      "step": 5625
    },
    {
      "epoch": 0.9527422958559926,
      "grad_norm": 0.5025708079338074,
      "learning_rate": 0.00010484306446169423,
      "loss": 0.5781,
      "step": 5650
    },
    {
      "epoch": 0.9569579697314615,
      "grad_norm": 0.5325255990028381,
      "learning_rate": 0.00010442119473506582,
      "loss": 0.5653,
      "step": 5675
    },
    {
      "epoch": 0.9611736436069306,
      "grad_norm": 0.436511367559433,
      "learning_rate": 0.0001039993250084374,
      "loss": 0.5914,
      "step": 5700
    },
    {
      "epoch": 0.9653893174823995,
      "grad_norm": 0.6475377082824707,
      "learning_rate": 0.00010357745528180897,
      "loss": 0.567,
      "step": 5725
    },
    {
      "epoch": 0.9696049913578686,
      "grad_norm": 0.49628663063049316,
      "learning_rate": 0.00010315558555518057,
      "loss": 0.5908,
      "step": 5750
    },
    {
      "epoch": 0.9738206652333375,
      "grad_norm": 0.5009115934371948,
      "learning_rate": 0.00010273371582855216,
      "loss": 0.5096,
      "step": 5775
    },
    {
      "epoch": 0.9780363391088065,
      "grad_norm": 0.43752309679985046,
      "learning_rate": 0.00010231184610192373,
      "loss": 0.6214,
      "step": 5800
    },
    {
      "epoch": 0.9822520129842756,
      "grad_norm": 0.5446152091026306,
      "learning_rate": 0.00010188997637529531,
      "loss": 0.6389,
      "step": 5825
    },
    {
      "epoch": 0.9864676868597445,
      "grad_norm": 0.35858139395713806,
      "learning_rate": 0.0001014681066486669,
      "loss": 0.5722,
      "step": 5850
    },
    {
      "epoch": 0.9906833607352136,
      "grad_norm": 0.4427186846733093,
      "learning_rate": 0.00010104623692203847,
      "loss": 0.5296,
      "step": 5875
    },
    {
      "epoch": 0.9948990346106825,
      "grad_norm": 0.7008196711540222,
      "learning_rate": 0.00010062436719541007,
      "loss": 0.5847,
      "step": 5900
    },
    {
      "epoch": 0.9991147084861515,
      "grad_norm": 0.5215916633605957,
      "learning_rate": 0.00010020249746878165,
      "loss": 0.5625,
      "step": 5925
    }
  ],
  "logging_steps": 25,
  "max_steps": 11862,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 5931,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.3157129762364672e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
