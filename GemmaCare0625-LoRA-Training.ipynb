{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e10e0715",
   "metadata": {},
   "source": [
    "# GemmaCare0625-LoRA: LoRA Fine-Tuning of Gemma-2B\n",
    "\n",
    "**Author:** Olisemeka Nmarkwe  \n",
    "\n",
    "This notebook fine-tunes Google's Gemma-2B model using LoRA (Low-Rank Adaptation) for efficient parameter updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8b84bb",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "\n",
    "Install all required libraries for the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b149b347",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet transformers datasets accelerate peft huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bda8584",
   "metadata": {},
   "source": [
    "## 2. Imports and Authentication\n",
    "\n",
    "Import necessary libraries and authenticate with Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921fd983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import pandas as pd\n",
    "\n",
    "# Log in to Hugging Face (interactive prompt)\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6a8aae",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "Set up all training parameters and model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c783d5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"google/gemma-2b\"\n",
    "MODEL_NAME = \"GemmaCare0625-LoRA\"\n",
    "HUB_REPO   = \"OliseNS/GemmaCare0625-LoRA\"\n",
    "EPOCHS     = 10\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "MAX_LENGTH = 1024\n",
    "\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"Base Model: {BASE_MODEL}\")\n",
    "print(f\"Model Name: {MODEL_NAME}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"Max Length: {MAX_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9174e5",
   "metadata": {},
   "source": [
    "## 4. Upload Data Files\n",
    "\n",
    "Upload your training and validation data files. Uncomment the appropriate section based on your platform:\n",
    "- **Colab**: Use the files.upload() method\n",
    "- **Kaggle**: Use the Kaggle dataset upload interface\n",
    "- **Local**: Ensure files are in the correct directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712e5e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab - uncomment the lines below:\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()  # select train.jsonl and validation.jsonl\n",
    "\n",
    "# For Kaggle or local environment, ensure files are in the working directory\n",
    "train_file = \"train.jsonl\"\n",
    "val_file   = \"validation.jsonl\"\n",
    "\n",
    "# Verify files exist\n",
    "import os\n",
    "print(f\"Training file exists: {os.path.exists(train_file)}\")\n",
    "print(f\"Validation file exists: {os.path.exists(val_file)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc972bf",
   "metadata": {},
   "source": [
    "## 5. Quick Data Inspection\n",
    "\n",
    "Examine the structure and content of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7932464",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inspecting training data...\"))\n",
    "df = pd.read_json(train_file, lines=True)\n",
    "print(f\"Rows: {len(df)}, Columns: {df.columns.tolist()}\")\n",
    "print(\"\\nFirst example:\")\n",
    "print(df.head(1).to_dict(orient='records')[0])\n",
    "\n",
    "print(\"\\nValidation data:\")\n",
    "df_val = pd.read_json(val_file, lines=True)\n",
    "print(f\"Validation rows: {len(df_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013d950b",
   "metadata": {},
   "source": [
    "## 6. Load Dataset\n",
    "\n",
    "Load the training and validation datasets using Hugging Face datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866a87dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\"train\": train_file, \"validation\": val_file}\n",
    ")\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset['validation'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f226a1cf",
   "metadata": {},
   "source": [
    "## 7. Initialize Tokenizer and Model\n",
    "\n",
    "Load the base Gemma-2B model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a5cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading tokenizer and model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    trust_remote_code=True,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Model loaded with {model.num_parameters():,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca8e088",
   "metadata": {},
   "source": [
    "## 8. Configure and Attach LoRA Adapter\n",
    "\n",
    "Set up LoRA configuration for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8737253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ca7295",
   "metadata": {},
   "source": [
    "## 9. Data Preprocessing\n",
    "\n",
    "Tokenize the dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e71bf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(examples):\n",
    "    combined = [p + c for p, c in zip(examples['prompt'], examples['completion'])]\n",
    "    tokens = tokenizer(\n",
    "        combined,\n",
    "        max_length=MAX_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    tokens['labels'] = tokens['input_ids'].copy()\n",
    "    return tokens\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized = dataset.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=['prompt', 'completion']\n",
    ")\n",
    "\n",
    "print(\"Tokenization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff14d50b",
   "metadata": {},
   "source": [
    "## 10. Setup Training Arguments and Trainer\n",
    "\n",
    "Configure the training parameters and initialize the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3060cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='checkpoints',\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=3,\n",
    "    logging_steps=50,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=HUB_REPO\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized['train'],\n",
    "    eval_dataset=tokenized['validation'],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2e4853",
   "metadata": {},
   "source": [
    "## 11. Start Training\n",
    "\n",
    "Begin the 10-epoch training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79294853",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting training of {MODEL_NAME}...\"))\n",
    "print(f\"Training for {EPOCHS} epochs with batch size {BATCH_SIZE}\")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66ba0b9",
   "metadata": {},
   "source": [
    "## 12. Evaluate and Save Model\n",
    "\n",
    "Evaluate the final model and save/upload to Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02df78a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "print(\"Performing final evaluation...\")\n",
    "metrics = trainer.evaluate()\n",
    "print(\"Final metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Push to Hub\n",
    "print(f\"\\nUploading {MODEL_NAME} to Hugging Face Hub...\")\n",
    "trainer.push_to_hub(commit_message=f\"Final {MODEL_NAME} model\")\n",
    "\n",
    "# Save locally\n",
    "print(f\"\\nSaving {MODEL_NAME} adapter locally...\")\n",
    "model.save_pretrained(f'{MODEL_NAME}-adapter')\n",
    "tokenizer.save_pretrained(f'{MODEL_NAME}-adapter')\n",
    "\n",
    "print(f\"\\n✅ All done! Your {MODEL_NAME} adapter is ready for use or sharing via Hugging Face.\")\n",
    "print(f\"🔗 Model available at: https://huggingface.co/{HUB_REPO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb06ac1e",
   "metadata": {},
   "source": [
    "## 13. Model Testing (Optional)\n",
    "\n",
    "Test the fine-tuned model with a sample prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55485cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Test the model\n",
    "test_prompt = \"What are the symptoms of diabetes?\"\n",
    "print(f\"Testing model with prompt: '{test_prompt}'\")\n",
    "\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"\\nModel response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eb4845",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook successfully fine-tuned the Gemma-2B model using LoRA for the GemmaCare0625 project. The model has been trained for 10 epochs and is now available for inference or further fine-tuning.\n",
    "\n",
    "**Key Features:**\n",
    "- ✅ LoRA fine-tuning for efficient parameter updates\n",
    "- ✅ 10-epoch training with evaluation\n",
    "- ✅ Automatic model upload to Hugging Face Hub\n",
    "- ✅ Compatible with Kaggle, Colab, and other platforms\n",
    "\n",
    "**Author:** Olisemeka Nmarkwe  \n",
    "**Model Repository:** [OliseNS/GemmaCare0625-LoRA](https://huggingface.co/OliseNS/GemmaCare0625-LoRA)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
